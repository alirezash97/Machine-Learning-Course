{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Annotations.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezash97/Machine-Learning-Course/blob/main/Final_Project/Annotations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvHWw5FNjHCB",
        "outputId": "652447bc-a0ba-4756-a647-85c2bc000ef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzvA1piVH8Ok"
      },
      "source": [
        "# !wget 'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/23870/1781260/compressed/train.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1612106913&Signature=GmMvvMnWZVZ91%2BVhJGQrB2r7z2w5s%2FKJtEaLsK8drRZ%2B4s5vCnfvbjsOg3ZijjSd%2Bf4l6d1dczcZJA0BIS%2FGomA66YjHsj2wKnC6JIApCZWQV290eQLCPTcjLOqjO%2BfaPpC7fVAmnRZPRfUVcar%2BBKZrJQ0QEUUY25%2BhsbhYP%2F2mLdHuAMbd9sg32O4Gp7uWeKnG3fPkf2KcSxvd9SLvh1WNwtmualXT3hhxG5jYATLzVXGQ4Y1UgBBwHnPGkG5Q18MQSgbXfImuFXEW9n0txsuUWk4KlH1ivL9MZJNNs0r6GJfMSksSzshWhg44bd%2BfnNwsPZ1Qgjutx%2F2No4tznQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.zip'\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# !wget 'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/23870/1781260/compressed/train_annotations.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1611897470&Signature=OteKBnljOwGhvVGphEn3gxjPtULu7rF6lCl8QhWUzknSMWdayANHoEab1p4ObT%2F7UeLmBGZ90Cp6%2B0imtPAhxAUpXzISZQvRZObQWPbUJW8SRTjJS2%2Bbp2ZOlXe%2Be0iXiUO%2FvPIULpj2EIKbBtuvByStii4jWfsk5Ju376bRHwxv7YQuDf2%2BbNoacXBQ1AfhV%2FM7XGiN6rgM%2BfVOAPHaEacXHk1vSXZcmfgs38VcP5BZVD6DQW5lVF2X%2Fm0rxfIFtCUrq0%2Btes5KnibXDifU3iY59z4%2FIAMCKhjIGXZ%2FddaSmnQaRCMkmMTJRtCASd4e6rll2AFCI1XFR5CL8Z%2FROw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain_annotations.csv.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE0z02ObIO_J"
      },
      "source": [
        "# !mkdir /content/trainset\r\n",
        "# !mkdir /content/trainset/data/\r\n",
        "# !mkdir /content/trainset/data/1/\r\n",
        "# !unzip '/content/train.zip' -d /content/trainset/data/1/\r\n",
        "# !unzip '/content/train_annotations.csv.zip' -d /content/trainset/annotations/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "4N1FWlaEJTw8",
        "outputId": "0cac872b-52e5-4411-8da6-1e406d37bc5c"
      },
      "source": [
        "import pandas as pd \r\n",
        "annotations = pd.read_csv('/content/trainset/annotations/train_annotations.csv')\r\n",
        "annotations.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>label</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12616281126973421762...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1487, 1279], [1477, 1168], [1472, 1052], [14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12616281126973421762...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1328, 7], [1347, 101], [1383, 193], [1400, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.72921907356394389969...</td>\n",
              "      <td>CVC - Borderline</td>\n",
              "      <td>[[801, 1207], [812, 1112], [823, 1023], [842, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.11697104485452001927...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1366, 961], [1411, 861], [1453, 751], [1508,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.87704688663091069148...</td>\n",
              "      <td>NGT - Normal</td>\n",
              "      <td>[[1862, 14], [1845, 293], [1801, 869], [1716, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    StudyInstanceUID  ...                                               data\n",
              "0  1.2.826.0.1.3680043.8.498.12616281126973421762...  ...  [[1487, 1279], [1477, 1168], [1472, 1052], [14...\n",
              "1  1.2.826.0.1.3680043.8.498.12616281126973421762...  ...  [[1328, 7], [1347, 101], [1383, 193], [1400, 2...\n",
              "2  1.2.826.0.1.3680043.8.498.72921907356394389969...  ...  [[801, 1207], [812, 1112], [823, 1023], [842, ...\n",
              "3  1.2.826.0.1.3680043.8.498.11697104485452001927...  ...  [[1366, 961], [1411, 861], [1453, 751], [1508,...\n",
              "4  1.2.826.0.1.3680043.8.498.87704688663091069148...  ...  [[1862, 14], [1845, 293], [1801, 869], [1716, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkPMdW9l_pHs",
        "outputId": "3133cbd5-acd7-49cc-ae63-58025711e966"
      },
      "source": [
        "len(annotations)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhInlCf07y-3"
      },
      "source": [
        "import re\r\n",
        "import ast\r\n",
        "import numpy as np\r\n",
        "def str2array(s):\r\n",
        "    # Remove space after [\r\n",
        "    s=re.sub('\\[ +', '[', s.strip())\r\n",
        "    # Replace commas and spaces\r\n",
        "    s=re.sub('[,\\s]+', ', ', s)\r\n",
        "    return np.array(ast.literal_eval(s))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltu_ndY1Kgkk",
        "outputId": "539c8fa7-dea3-4a25-f618-be4cb95df38b"
      },
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "\r\n",
        "msk = np.random.rand(len(annotations)) < 0.3\r\n",
        "train_samples = annotations[msk]\r\n",
        "validation_samples = annotations[~msk]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "sample = annotations.iloc[1, :]\r\n",
        "landmarks = sample['data']\r\n",
        "print(landmarks)\r\n",
        "landmarks = np.array(str2array(landmarks))\r\n",
        "print(type(landmarks))\r\n",
        "# landmarks = np.array(list(landmarks))\r\n",
        "print(landmarks.shape)\r\n",
        "print(landmarks)\r\n",
        "\r\n",
        "# print('Image name: {}'.format(img_name))\r\n",
        "# print('Landmarks shape: {}'.format(landmarks.shape))\r\n",
        "# print('First 4 Landmarks: {}'.format(landmarks[:4]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1328, 7], [1347, 101], [1383, 193], [1400, 267], [1411, 366], [1400, 428], [1387, 545], [1394, 640], [1400, 707], [1417, 783], [1432, 852], [1462, 953], [1457, 1006]]\n",
            "<class 'numpy.ndarray'>\n",
            "(13, 2)\n",
            "[[1328    7]\n",
            " [1347  101]\n",
            " [1383  193]\n",
            " [1400  267]\n",
            " [1411  366]\n",
            " [1400  428]\n",
            " [1387  545]\n",
            " [1394  640]\n",
            " [1400  707]\n",
            " [1417  783]\n",
            " [1432  852]\n",
            " [1462  953]\n",
            " [1457 1006]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDO0eWSppIjD"
      },
      "source": [
        "from PIL import Image\r\n",
        "import random\r\n",
        "import torch.nn.functional as F\r\n",
        "from math import cos, sin, radians\r\n",
        "import imutils\r\n",
        "import cv2\r\n",
        "\r\n",
        "\r\n",
        "class RANZCRDataset():\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, csv_file='/content/trainset/train.csv', root_dir='/content/trainset/data/1', transform=None, images_name=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.Images_name = images_name\r\n",
        "        self.root_dir = root_dir\r\n",
        "        self.transform = transform\r\n",
        "        \r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.Images_name)\r\n",
        "\r\n",
        "    #############\r\n",
        "\r\n",
        "\r\n",
        "    # def get_rot_mat(self, theta):\r\n",
        "\r\n",
        "    #   theta = torch.tensor(theta)\r\n",
        "    #   return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\r\n",
        "    #                         [torch.sin(theta), torch.cos(theta), 0]])\r\n",
        "\r\n",
        "\r\n",
        "    # def rot_img_landmark(self, x, landmarks, theta, dtype):\r\n",
        "    #     rot_mat = self.get_rot_mat(theta)[None, ...].type(dtype).repeat(x.shape[0],1,1)\r\n",
        "    #     grid = F.affine_grid(rot_mat, x.size()).type(dtype)\r\n",
        "    #     image = F.grid_sample(x, grid)\r\n",
        "    #     landmarks = landmarks - 0.5\r\n",
        "    #     new_landmarks = np.matmul(landmarks, transformation_matrix)\r\n",
        "    #     new_landmarks = new_landmarks + 0.5\r\n",
        "    #     return image, new_landmarks\r\n",
        "\r\n",
        "\r\n",
        "    ##############\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        img_name = os.path.join(self.root_dir,\r\n",
        "                                self.Images_name.iloc[idx, 0])\r\n",
        "        image = Image.open(img_name + '.jpg').convert('RGB')\r\n",
        "        labels = self.Images_name.iloc[idx, -1]\r\n",
        "        labels = torch.from_numpy(str2array(labels))\r\n",
        "        sample = {'image': image, 'label': labels }\r\n",
        "\r\n",
        "\r\n",
        "        if self.transform:\r\n",
        "\r\n",
        "          \r\n",
        "          tmp = np.zeros((100, 2))\r\n",
        "\r\n",
        "          for i in range(sample['label'].shape[0]):\r\n",
        "\r\n",
        "            tmp[i, 0] = (centerCrop_value / np.array(image).shape[1]) * np.array(sample['label'])[i, 0]\r\n",
        "            tmp[i, 1] = (centerCrop_value / np.array(image).shape[0]) * np.array(sample['label'])[i, 1]\r\n",
        "             \r\n",
        "\r\n",
        "          sample['label'] = torch.from_numpy(tmp).type(torch.float16)\r\n",
        "\r\n",
        "\r\n",
        "          sample['image'] = self.transform(sample['image'])\r\n",
        "\r\n",
        "\r\n",
        "          # # random rotation\r\n",
        "          # image, landmark = self.rot_img_landmark(sample['image'], sample['label'], np.pi/2, dtype= torch.FloatTensor)\r\n",
        "          # print(type(image), image.shape)\r\n",
        "          # print(type(landmark), landmark.shape)\r\n",
        "          \r\n",
        "\r\n",
        "        return sample\r\n",
        "\r\n",
        "# my_dataset = RANZCRDataset\r\n",
        "# my_dataset.__getitem__(self, 4)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojvstcXDbmN"
      },
      "source": [
        "\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchvision import transforms, utils\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import os\r\n",
        "\r\n",
        "batch_size = 8\r\n",
        "mean = np.array([0.4823, 0.4823, 0.4823])\r\n",
        "std = np.array([0.191473164, 0.191473164, 0.191473164])\r\n",
        "\r\n",
        "centerCrop_value = 244\r\n",
        "transform = transforms.Compose([transforms.ToTensor(),\r\n",
        "                                transforms.Resize((256, 256)),\r\n",
        "                                transforms.CenterCrop(centerCrop_value),\r\n",
        "                                transforms.Normalize(mean, std)])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "trainset = RANZCRDataset(csv_file='/content/trainset/annotations/train_annotations.csv',\r\n",
        "                                    root_dir='/content/trainset/data/1', transform=transform, images_name=train_samples)\r\n",
        "\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(trainset,\r\n",
        "                         batch_size=batch_size,\r\n",
        "                         num_workers=0,\r\n",
        "                         shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "validation_set = RANZCRDataset(csv_file='/content/trainset/annotations/train_annotations.csv',\r\n",
        "                                    root_dir='/content/trainset/data/1', transform=transform, images_name=validation_samples)\r\n",
        "\r\n",
        "\r\n",
        "validation_loader = torch.utils.data.DataLoader(trainset,\r\n",
        "                         batch_size=batch_size,\r\n",
        "                         num_workers=0,\r\n",
        "                         shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "cHNoNytaOALb",
        "outputId": "e83aaee7-2a21-4997-946d-bf05780c6af4"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "def imshow(img, landmarks):\r\n",
        "    npimg = img.numpy()\r\n",
        "    npimg = ((npimg * std[0]) + mean[0]) # unnormalize\r\n",
        "    plt.imshow((np.transpose(npimg, (1, 2, 0)) * 255).astype(np.uint8))\r\n",
        "    for i in range(landmarks.shape[0]):\r\n",
        "      landmarks[i, :, 0] = landmarks[i, :, 0] + (centerCrop_value*i)\r\n",
        "    plt.scatter(landmarks[:, :, 0], landmarks[:, :, 1], s=10, marker='.', c='r')\r\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "# get some random training images\r\n",
        "dataiter = iter(train_loader)\r\n",
        "sample = dataiter.next()\r\n",
        "\r\n",
        "imshow(torchvision.utils.make_grid(sample['image']), sample['label'])\r\n",
        "print(sample['label'].shape)\r\n",
        "print(sample['image'].shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 100, 2])\n",
            "torch.Size([8, 3, 244, 244])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05o0AD8DzDyf"
      },
      "source": [
        "# class Network(nn.Module):\r\n",
        "  \r\n",
        "#   def __init__(self):\r\n",
        "#     super(Network, self).__init__()\r\n",
        "#     self.model = model\r\n",
        "#     self.conv1 = nn.Conv2d(3, 3, 5)\r\n",
        "#     self.conv2 = nn.Conv2d(3, 3, 1)\r\n",
        "#     self.pool2 = nn.MaxPool2d(2, 2)\r\n",
        "#     self.sigmoid = nn.Sigmoid()\r\n",
        "#     self.fc_final = nn.Linear(1000, 11)\r\n",
        "\r\n",
        "#   def forward(self, x):\r\n",
        "\r\n",
        "#     x = self.pool2(F.relu(self.conv1(x)))\r\n",
        "#     x = self.pool2(F.relu(self.conv2(x)))\r\n",
        "#     x = self.model(x)\r\n",
        "#     x = self.sigmoid(self.fc_final(x))\r\n",
        "#     return x\r\n",
        "\r\n",
        "# Network = Network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdKfqyGawGY8"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "from torchvision import models\r\n",
        "from torch import optim\r\n",
        "import time\r\n",
        "\r\n",
        "model = models.resnet152(pretrained=False)\r\n",
        "\r\n",
        "class Network(nn.Module):\r\n",
        "    def __init__(self,num_classes=200):\r\n",
        "        super().__init__()\r\n",
        "        self.model_name='resnet18'\r\n",
        "        self.model=model\r\n",
        "        self.fc_final = nn.Linear(1000, 200)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.model(x)\r\n",
        "        x = F.relu(self.fc_final(x))\r\n",
        "        return x"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVUdllE-2WHE"
      },
      "source": [
        "Network = Network()\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = optim.Adam(Network.parameters(), lr=0.0025, weight_decay=1e-05)\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BscbT2lMMMHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c8b589-b369-4810-aabb-d0a2ca037e71"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print(count_parameters(model))\r\n",
        "print(count_parameters(Network))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60192808\n",
            "60393008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yOUc5MLn2H8B",
        "outputId": "4381a489-babb-435e-ae94-f9c4a26cc9b5"
      },
      "source": [
        "\r\n",
        "for epoch in range(40):\r\n",
        "\r\n",
        "  running_loss = 0.0\r\n",
        "  best_validation_loss = 1000000000000\r\n",
        "\r\n",
        "  for batch_number, data in enumerate(train_loader, 0):\r\n",
        "    \r\n",
        "  \r\n",
        "    inputs = data['image'].float()\r\n",
        "    label_temp = data['label']\r\n",
        "    label = np.zeros((batch_size, 200))\r\n",
        "    for i in range(batch_size):\r\n",
        "      if label_temp.shape[0] == batch_size:\r\n",
        "        label[i, :100] = label_temp[i, :, 0]\r\n",
        "        label[i, 100:] = label_temp[i, :, 1]\r\n",
        "      else:\r\n",
        "        pass\r\n",
        "\r\n",
        "    label = torch.from_numpy(label).float()\r\n",
        "    optimizer.zero_grad()\r\n",
        "    outputs = Network(inputs)\r\n",
        "    loss = criterion(outputs, label)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    running_loss += loss.item()\r\n",
        "    if (batch_number % 10 == 0) and (batch_number > 9):\r\n",
        "      print('[epoch: %d, batch: %5d] training loss: %.3f' %( epoch + 1, batch_number, (running_loss/10)))\r\n",
        "      \r\n",
        "      dataiter = iter(validation_loader)\r\n",
        "      sample = dataiter.next()\r\n",
        "      validation_inputs = sample['image'].float()\r\n",
        "      validation_label_temp = data['label']\r\n",
        "      validation_label = np.zeros((batch_size, 200))\r\n",
        "      for i in range(batch_size):\r\n",
        "        validation_label[i, :100] = validation_label_temp[i, :, 0]\r\n",
        "        validation_label[i, 100:] = validation_label_temp[i, :, 1]\r\n",
        "\r\n",
        "      validation_label = torch.from_numpy(validation_label).float()\r\n",
        "      validation_outputs = Network(validation_inputs)\r\n",
        "      validation_loss = criterion(validation_outputs, validation_label)\r\n",
        "      \r\n",
        "\r\n",
        "      print('[epoch: %d, batch: %5d] <validation 10 random sample> loss: %.3f' %( epoch + 1, batch_number, validation_loss))\r\n",
        "\r\n",
        "\r\n",
        "      running_loss = 0.0\r\n",
        "\r\n",
        "  try:\r\n",
        "    if validation_loss < best_validation_loss:\r\n",
        "      torch.save(Network.state_dict(), '/content/drive/MyDrivelandmarks.pth')\r\n",
        "      best_validation_loss = validation_loss\r\n",
        "  except ValueError:\r\n",
        "    pass\r\n",
        "  \r\n",
        "    \r\n",
        "print('Finished Training Network')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch: 1, batch:    10] training loss: 1570.635\n",
            "[epoch: 1, batch:    10] <validation 10 random sample> loss: 843.638\n",
            "[epoch: 1, batch:    20] training loss: 1057.810\n",
            "[epoch: 1, batch:    20] <validation 10 random sample> loss: 1391.703\n",
            "[epoch: 1, batch:    30] training loss: 960.650\n",
            "[epoch: 1, batch:    30] <validation 10 random sample> loss: 652.755\n",
            "[epoch: 1, batch:    40] training loss: 1222.799\n",
            "[epoch: 1, batch:    40] <validation 10 random sample> loss: 949.858\n",
            "[epoch: 1, batch:    50] training loss: 938.923\n",
            "[epoch: 1, batch:    50] <validation 10 random sample> loss: 509.950\n",
            "[epoch: 1, batch:    60] training loss: 1030.899\n",
            "[epoch: 1, batch:    60] <validation 10 random sample> loss: 762.595\n",
            "[epoch: 1, batch:    70] training loss: 1215.795\n",
            "[epoch: 1, batch:    70] <validation 10 random sample> loss: 1417.153\n",
            "[epoch: 1, batch:    80] training loss: 893.683\n",
            "[epoch: 1, batch:    80] <validation 10 random sample> loss: 1039.480\n",
            "[epoch: 1, batch:    90] training loss: 1018.521\n",
            "[epoch: 1, batch:    90] <validation 10 random sample> loss: 649.577\n",
            "[epoch: 1, batch:   100] training loss: 1074.565\n",
            "[epoch: 1, batch:   100] <validation 10 random sample> loss: 891.194\n",
            "[epoch: 1, batch:   110] training loss: 1119.393\n",
            "[epoch: 1, batch:   110] <validation 10 random sample> loss: 1316.683\n",
            "[epoch: 1, batch:   120] training loss: 950.886\n",
            "[epoch: 1, batch:   120] <validation 10 random sample> loss: 606.376\n",
            "[epoch: 1, batch:   130] training loss: 927.284\n",
            "[epoch: 1, batch:   130] <validation 10 random sample> loss: 843.970\n",
            "[epoch: 1, batch:   140] training loss: 993.847\n",
            "[epoch: 1, batch:   140] <validation 10 random sample> loss: 614.966\n",
            "[epoch: 1, batch:   150] training loss: 985.102\n",
            "[epoch: 1, batch:   150] <validation 10 random sample> loss: 1088.235\n",
            "[epoch: 1, batch:   160] training loss: 760.670\n",
            "[epoch: 1, batch:   160] <validation 10 random sample> loss: 1573.438\n",
            "[epoch: 1, batch:   170] training loss: 992.551\n",
            "[epoch: 1, batch:   170] <validation 10 random sample> loss: 835.367\n",
            "[epoch: 1, batch:   180] training loss: 868.540\n",
            "[epoch: 1, batch:   180] <validation 10 random sample> loss: 857.880\n",
            "[epoch: 1, batch:   190] training loss: 1217.459\n",
            "[epoch: 1, batch:   190] <validation 10 random sample> loss: 983.821\n",
            "[epoch: 1, batch:   200] training loss: 776.305\n",
            "[epoch: 1, batch:   200] <validation 10 random sample> loss: 815.152\n",
            "[epoch: 1, batch:   210] training loss: 826.282\n",
            "[epoch: 1, batch:   210] <validation 10 random sample> loss: 1381.476\n",
            "[epoch: 1, batch:   220] training loss: 1066.733\n",
            "[epoch: 1, batch:   220] <validation 10 random sample> loss: 932.474\n",
            "[epoch: 1, batch:   230] training loss: 1093.573\n",
            "[epoch: 1, batch:   230] <validation 10 random sample> loss: 1333.949\n",
            "[epoch: 1, batch:   240] training loss: 1258.374\n",
            "[epoch: 1, batch:   240] <validation 10 random sample> loss: 1097.282\n",
            "[epoch: 1, batch:   250] training loss: 1196.604\n",
            "[epoch: 1, batch:   250] <validation 10 random sample> loss: 1564.378\n",
            "[epoch: 1, batch:   260] training loss: 1144.250\n",
            "[epoch: 1, batch:   260] <validation 10 random sample> loss: 550.169\n",
            "[epoch: 1, batch:   270] training loss: 1079.524\n",
            "[epoch: 1, batch:   270] <validation 10 random sample> loss: 941.126\n",
            "[epoch: 1, batch:   280] training loss: 1220.965\n",
            "[epoch: 1, batch:   280] <validation 10 random sample> loss: 1083.836\n",
            "[epoch: 1, batch:   290] training loss: 1068.488\n",
            "[epoch: 1, batch:   290] <validation 10 random sample> loss: 901.171\n",
            "[epoch: 1, batch:   300] training loss: 1102.636\n",
            "[epoch: 1, batch:   300] <validation 10 random sample> loss: 488.279\n",
            "[epoch: 1, batch:   310] training loss: 920.370\n",
            "[epoch: 1, batch:   310] <validation 10 random sample> loss: 739.994\n",
            "[epoch: 1, batch:   320] training loss: 893.570\n",
            "[epoch: 1, batch:   320] <validation 10 random sample> loss: 887.417\n",
            "[epoch: 1, batch:   330] training loss: 1100.153\n",
            "[epoch: 1, batch:   330] <validation 10 random sample> loss: 941.371\n",
            "[epoch: 1, batch:   340] training loss: 990.339\n",
            "[epoch: 1, batch:   340] <validation 10 random sample> loss: 585.276\n",
            "[epoch: 1, batch:   350] training loss: 1187.670\n",
            "[epoch: 1, batch:   350] <validation 10 random sample> loss: 688.545\n",
            "[epoch: 1, batch:   360] training loss: 944.107\n",
            "[epoch: 1, batch:   360] <validation 10 random sample> loss: 1339.556\n",
            "[epoch: 1, batch:   370] training loss: 1090.649\n",
            "[epoch: 1, batch:   370] <validation 10 random sample> loss: 1655.709\n",
            "[epoch: 1, batch:   380] training loss: 954.594\n",
            "[epoch: 1, batch:   380] <validation 10 random sample> loss: 1745.225\n",
            "[epoch: 1, batch:   390] training loss: 950.777\n",
            "[epoch: 1, batch:   390] <validation 10 random sample> loss: 987.713\n",
            "[epoch: 1, batch:   400] training loss: 984.316\n",
            "[epoch: 1, batch:   400] <validation 10 random sample> loss: 1007.875\n",
            "[epoch: 1, batch:   410] training loss: 1079.439\n",
            "[epoch: 1, batch:   410] <validation 10 random sample> loss: 876.856\n",
            "[epoch: 1, batch:   420] training loss: 1278.245\n",
            "[epoch: 1, batch:   420] <validation 10 random sample> loss: 882.205\n",
            "[epoch: 1, batch:   430] training loss: 1197.446\n",
            "[epoch: 1, batch:   430] <validation 10 random sample> loss: 1785.441\n",
            "[epoch: 1, batch:   440] training loss: 786.887\n",
            "[epoch: 1, batch:   440] <validation 10 random sample> loss: 784.214\n",
            "[epoch: 1, batch:   450] training loss: 950.064\n",
            "[epoch: 1, batch:   450] <validation 10 random sample> loss: 1199.849\n",
            "[epoch: 1, batch:   460] training loss: 938.605\n",
            "[epoch: 1, batch:   460] <validation 10 random sample> loss: 621.195\n",
            "[epoch: 1, batch:   470] training loss: 1154.447\n",
            "[epoch: 1, batch:   470] <validation 10 random sample> loss: 2238.375\n",
            "[epoch: 1, batch:   480] training loss: 939.721\n",
            "[epoch: 1, batch:   480] <validation 10 random sample> loss: 1188.595\n",
            "[epoch: 1, batch:   490] training loss: 1223.073\n",
            "[epoch: 1, batch:   490] <validation 10 random sample> loss: 471.210\n",
            "[epoch: 1, batch:   500] training loss: 968.992\n",
            "[epoch: 1, batch:   500] <validation 10 random sample> loss: 853.569\n",
            "[epoch: 1, batch:   510] training loss: 968.441\n",
            "[epoch: 1, batch:   510] <validation 10 random sample> loss: 589.009\n",
            "[epoch: 1, batch:   520] training loss: 1092.329\n",
            "[epoch: 1, batch:   520] <validation 10 random sample> loss: 765.904\n",
            "[epoch: 1, batch:   530] training loss: 936.838\n",
            "[epoch: 1, batch:   530] <validation 10 random sample> loss: 1119.644\n",
            "[epoch: 1, batch:   540] training loss: 1260.648\n",
            "[epoch: 1, batch:   540] <validation 10 random sample> loss: 913.887\n",
            "[epoch: 1, batch:   550] training loss: 1011.549\n",
            "[epoch: 1, batch:   550] <validation 10 random sample> loss: 793.393\n",
            "[epoch: 1, batch:   560] training loss: 1039.654\n",
            "[epoch: 1, batch:   560] <validation 10 random sample> loss: 1804.051\n",
            "[epoch: 1, batch:   570] training loss: 960.960\n",
            "[epoch: 1, batch:   570] <validation 10 random sample> loss: 1123.133\n",
            "[epoch: 1, batch:   580] training loss: 986.620\n",
            "[epoch: 1, batch:   580] <validation 10 random sample> loss: 1484.518\n",
            "[epoch: 1, batch:   590] training loss: 1254.847\n",
            "[epoch: 1, batch:   590] <validation 10 random sample> loss: 1705.770\n",
            "[epoch: 1, batch:   600] training loss: 1067.251\n",
            "[epoch: 1, batch:   600] <validation 10 random sample> loss: 940.811\n",
            "[epoch: 1, batch:   610] training loss: 1003.809\n",
            "[epoch: 1, batch:   610] <validation 10 random sample> loss: 1084.249\n",
            "[epoch: 1, batch:   620] training loss: 1050.724\n",
            "[epoch: 1, batch:   620] <validation 10 random sample> loss: 1466.218\n",
            "[epoch: 1, batch:   630] training loss: 1147.609\n",
            "[epoch: 1, batch:   630] <validation 10 random sample> loss: 1138.635\n",
            "[epoch: 1, batch:   640] training loss: 1020.582\n",
            "[epoch: 1, batch:   640] <validation 10 random sample> loss: 1766.898\n",
            "[epoch: 1, batch:   650] training loss: 948.254\n",
            "[epoch: 1, batch:   650] <validation 10 random sample> loss: 950.143\n",
            "[epoch: 1, batch:   660] training loss: 967.823\n",
            "[epoch: 1, batch:   660] <validation 10 random sample> loss: 1626.906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([8, 200])) that is different to the input size (torch.Size([7, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3ef34479cce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (8) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "fGd8r4eTwbJW",
        "outputId": "b977dc52-e9d8-40b3-dfda-afa0f5cf040a"
      },
      "source": [
        "# network = Network()\r\n",
        "# network.cuda()    \r\n",
        "\r\n",
        "# criterion = nn.MSELoss()\r\n",
        "# optimizer = optim.Adam(network.parameters(), lr=0.0001)\r\n",
        "\r\n",
        "# loss_min = np.inf\r\n",
        "# num_epochs = 10\r\n",
        "\r\n",
        "# start_time = time.time()\r\n",
        "# for epoch in range(1,num_epochs+1):\r\n",
        "    \r\n",
        "#     loss_train = 0\r\n",
        "#     loss_valid = 0\r\n",
        "#     running_loss = 0\r\n",
        "    \r\n",
        "#     network.train()\r\n",
        "#     for step in range(1,len(loader)+1):\r\n",
        "\r\n",
        "#         sample = next(iter(loader))\r\n",
        "#         images, landmarks = sample['image'], sample['label']\r\n",
        "        \r\n",
        "#         images = images.cuda()\r\n",
        "#         landmarks = landmarks.view(landmarks.size(0),-1).cuda() \r\n",
        "        \r\n",
        "#         predictions = network(images)\r\n",
        "        \r\n",
        "#         # clear all the gradients before calculating them\r\n",
        "#         optimizer.zero_grad()\r\n",
        "        \r\n",
        "#         # find the loss for the current step\r\n",
        "#         loss_train_step = criterion(predictions, landmarks)\r\n",
        "        \r\n",
        "#         # calculate the gradients\r\n",
        "#         loss_train_step.backward()\r\n",
        "        \r\n",
        "#         # update the parameters\r\n",
        "#         optimizer.step()\r\n",
        "        \r\n",
        "#         loss_train += loss_train_step.item()\r\n",
        "#         running_loss = loss_train/step\r\n",
        "        \r\n",
        "#         # print_overwrite(step, len(train_loader), running_loss, 'train')\r\n",
        "        \r\n",
        "#     network.eval() \r\n",
        "#     with torch.no_grad():\r\n",
        "        \r\n",
        "#         for step in range(1,  3): #len(loader)+1):\r\n",
        "            \r\n",
        "#             images, landmarks = next(iter(loader))\r\n",
        "        \r\n",
        "#             images = images.cuda()\r\n",
        "#             landmarks = landmarks.view(landmarks.size(0),-1).cuda()\r\n",
        "        \r\n",
        "#             predictions = network(images)\r\n",
        "\r\n",
        "#             # find the loss for the current step\r\n",
        "#             loss_valid_step = criterion(predictions, landmarks)\r\n",
        "\r\n",
        "#             loss_valid += loss_valid_step.item()\r\n",
        "#             running_loss = loss_valid/step\r\n",
        "\r\n",
        "#             print_overwrite(step, len(valid_loader), running_loss, 'valid')\r\n",
        "    \r\n",
        "#     loss_train /= len(train_loader)\r\n",
        "#     loss_valid /= len(valid_loader)\r\n",
        "    \r\n",
        "#     print('\\n--------------------------------------------------')\r\n",
        "#     print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\r\n",
        "#     print('--------------------------------------------------')\r\n",
        "    \r\n",
        "#     if loss_valid < loss_min:\r\n",
        "#         loss_min = loss_valid\r\n",
        "#         torch.save(network.state_dict(), '/content/face_landmarks.pth') \r\n",
        "#         print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\r\n",
        "#         print('Model Saved\\n')\r\n",
        "     \r\n",
        "# print('Training Complete')\r\n",
        "# print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-372-1d4fbc5f89c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-367-4dadf7cc0547>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenterCrop_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenterCrop_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJIX-wbgMO1C"
      },
      "source": [
        "# find dataset mean and std\r\n",
        "\r\n",
        "# import torch\r\n",
        "# from torch import Tensor\r\n",
        "# from typing import Iterable\r\n",
        "# from fastprogress import progress_bar\r\n",
        "\r\n",
        "# class RunningStatistics:\r\n",
        "#     '''Records mean and variance of the final `n_dims` dimension over other dimensions across items. So collecting across `(l,m,n,o)` sized\r\n",
        "#        items with `n_dims=1` will collect `(l,m,n)` sized statistics while with `n_dims=2` the collected statistics will be of size `(l,m)`.\r\n",
        "#        Uses the algorithm from Chan, Golub, and LeVeque in \"Algorithms for computing the sample variance: analysis and recommendations\":\r\n",
        "#        `variance = variance1 + variance2 + n/(m*(m+n)) * pow(((m/n)*t1 - t2), 2)`\r\n",
        "#        This combines the variance for 2 blocks: block 1 having `n` elements with `variance1` and a sum of `t1` and block 2 having `m` elements\r\n",
        "#        with `variance2` and a sum of `t2`. The algorithm is proven to be numerically stable but there is a reasonable loss of accuracy (~0.1% error).\r\n",
        "#        Note that collecting minimum and maximum values is reasonably innefficient, adding about 80% to the running time, and hence is disabled by default.\r\n",
        "#     '''\r\n",
        "#     def __init__(self, n_dims:int=2, record_range=False):\r\n",
        "#         self._n_dims,self._range = n_dims,record_range\r\n",
        "#         self.n,self.sum,self.min,self.max = 0,None,None,None\r\n",
        "    \r\n",
        "#     def update(self, data:Tensor):\r\n",
        "\r\n",
        "#         data = data.view(*list(data.shape[:-self._n_dims]) + [-1])\r\n",
        "#         with torch.no_grad():\r\n",
        "#             new_n,new_var,new_sum = data.shape[-1],data.var(-1),data.sum(-1)\r\n",
        "#             if self.n == 0:\r\n",
        "#                 self.n = new_n\r\n",
        "#                 self._shape = data.shape[:-1]\r\n",
        "#                 self.sum = new_sum\r\n",
        "#                 self._nvar = new_var.mul_(new_n)\r\n",
        "#                 if self._range:\r\n",
        "#                     self.min = data.min(-1)[0]\r\n",
        "#                     self.max = data.max(-1)[0]\r\n",
        "#             else:\r\n",
        "#                 # assert data.shape[:-1] == self._shape, f\"Mismatched shapes, expected {self._shape} but got {data.shape[:-1]}.\"\r\n",
        "#                 ratio = self.n / new_n\r\n",
        "#                 t = (self.sum / ratio).sub_(new_sum).pow_(2)\r\n",
        "#                 self._nvar.add_(new_n, new_var).add_(ratio / (self.n + new_n), t)\r\n",
        "#                 self.sum.add_(new_sum)\r\n",
        "#                 self.n += new_n\r\n",
        "#                 if self._range:\r\n",
        "#                     self.min = torch.min(self.min, data.min(-1)[0])\r\n",
        "#                     self.max = torch.max(self.max, data.max(-1)[0])\r\n",
        "\r\n",
        "#     @property\r\n",
        "#     def mean(self): return self.sum / self.n if self.n > 0 else None\r\n",
        "#     @property\r\n",
        "#     def var(self): return self._nvar / self.n if self.n > 0 else None\r\n",
        "#     @property\r\n",
        "#     def std(self): return self.var.sqrt() if self.n > 0 else None\r\n",
        "\r\n",
        "#     def __repr__(self):\r\n",
        "#         def _fmt_t(t:Tensor):\r\n",
        "#             if t.numel() > 5: return f\"tensor of ({','.join(map(str,t.shape))})\"\r\n",
        "#             def __fmt_t(t:Tensor):\r\n",
        "#                 return '[' + ','.join([f\"{v:.3g}\" if v.ndim==0 else __fmt_t(v) for v in t]) + ']'\r\n",
        "#             return __fmt_t(t)\r\n",
        "#         rng_str = f\", min={_fmt_t(self.min)}, max={_fmt_t(self.max)}\" if self._range else \"\"\r\n",
        "#         return f\"RunningStatistics(n={self.n}, mean={_fmt_t(self.mean)}, std={_fmt_t(self.std)}{rng_str})\"\r\n",
        "\r\n",
        "# def collect_stats(items:Iterable, n_dims:int=2, record_range:bool=False):\r\n",
        "#     stats = RunningStatistics(n_dims, record_range)\r\n",
        "#     for it in progress_bar(items.next()):\r\n",
        "#         it = it.float()\r\n",
        "#         if hasattr(it, 'data'):\r\n",
        "#             stats.update(it.data)\r\n",
        "#         else:\r\n",
        "#             stats.update(it)\r\n",
        "#     return stats\r\n",
        "\r\n",
        "# dd = RunningStatistics\r\n",
        "# stats = collect_stats(dataiter)\r\n",
        "# stats\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZXTf5UJDsbO",
        "outputId": "37e138e7-9b65-4653-d0f3-ab3d22356726"
      },
      "source": [
        "# stats.mean.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5057, 0.5057, 0.5057])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGmx2VtMJKQi",
        "outputId": "be0872a5-6dcd-43c5-e056-bc4732399513"
      },
      "source": [
        "# stats.std.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1902, 0.1902, 0.1902])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOCxit_OJ787"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}