{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Annotations.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezash97/Machine-Learning-Course/blob/main/Final_Project/Annotations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvHWw5FNjHCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc114178-5996-4519-8a4b-91c530e879f7"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzvA1piVH8Ok"
      },
      "source": [
        "# !wget 'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/23870/1781260/compressed/train.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1612106913&Signature=GmMvvMnWZVZ91%2BVhJGQrB2r7z2w5s%2FKJtEaLsK8drRZ%2B4s5vCnfvbjsOg3ZijjSd%2Bf4l6d1dczcZJA0BIS%2FGomA66YjHsj2wKnC6JIApCZWQV290eQLCPTcjLOqjO%2BfaPpC7fVAmnRZPRfUVcar%2BBKZrJQ0QEUUY25%2BhsbhYP%2F2mLdHuAMbd9sg32O4Gp7uWeKnG3fPkf2KcSxvd9SLvh1WNwtmualXT3hhxG5jYATLzVXGQ4Y1UgBBwHnPGkG5Q18MQSgbXfImuFXEW9n0txsuUWk4KlH1ivL9MZJNNs0r6GJfMSksSzshWhg44bd%2BfnNwsPZ1Qgjutx%2F2No4tznQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.zip'\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# !wget 'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/23870/1781260/compressed/train_annotations.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1612172751&Signature=mi0KGA%2FqdVG2LrVp2K%2BTofodkKYLyN5lkpYA4oNWQr5blgQtMuxDe1fu7WCS1ICd0HQ5fvAETb6xSS97OXfygJ9iplTVxpq2A9Q6SnaN6LcvVIYPgA9v%2Bc259suwSnGs%2FTgiyT%2BmHOnpgQ13nsFNXpP%2FhjqIo%2F9lkoSKN1BwPW3Woksi%2BE0UkusNBC5hrNO58z5FnVvaH3yGnozFAjzPwrY0M%2Bfh7viGX1%2FO%2BZauz6imQTyGoCcI2Znw3RvVM49bRBjPcwUaD6icFxb%2ByHO2QhV1i4BexBEnpZuqVgVd%2FaKfaiVSvlIMoRe76H%2Bsi7DW2ee6dPyC2F7h9ZdTk2yb4Q%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain_annotations.csv.zip'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE0z02ObIO_J"
      },
      "source": [
        "# !mkdir /content/trainset\r\n",
        "# !mkdir /content/trainset/data/\r\n",
        "# !mkdir /content/trainset/data/1/\r\n",
        "# !unzip '/content/train.zip' -d /content/trainset/data/1/\r\n",
        "# !unzip '/content/train_annotations.csv.zip' -d /content/trainset/annotations/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "4N1FWlaEJTw8",
        "outputId": "1866b180-ef15-48ff-b20a-b7f1b90b235d"
      },
      "source": [
        "import pandas as pd \r\n",
        "annotations = pd.read_csv('/content/trainset/annotations/train_annotations.csv')\r\n",
        "annotations.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>label</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12616281126973421762...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1487, 1279], [1477, 1168], [1472, 1052], [14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12616281126973421762...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1328, 7], [1347, 101], [1383, 193], [1400, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.72921907356394389969...</td>\n",
              "      <td>CVC - Borderline</td>\n",
              "      <td>[[801, 1207], [812, 1112], [823, 1023], [842, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.11697104485452001927...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1366, 961], [1411, 861], [1453, 751], [1508,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.87704688663091069148...</td>\n",
              "      <td>NGT - Normal</td>\n",
              "      <td>[[1862, 14], [1845, 293], [1801, 869], [1716, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    StudyInstanceUID  ...                                               data\n",
              "0  1.2.826.0.1.3680043.8.498.12616281126973421762...  ...  [[1487, 1279], [1477, 1168], [1472, 1052], [14...\n",
              "1  1.2.826.0.1.3680043.8.498.12616281126973421762...  ...  [[1328, 7], [1347, 101], [1383, 193], [1400, 2...\n",
              "2  1.2.826.0.1.3680043.8.498.72921907356394389969...  ...  [[801, 1207], [812, 1112], [823, 1023], [842, ...\n",
              "3  1.2.826.0.1.3680043.8.498.11697104485452001927...  ...  [[1366, 961], [1411, 861], [1453, 751], [1508,...\n",
              "4  1.2.826.0.1.3680043.8.498.87704688663091069148...  ...  [[1862, 14], [1845, 293], [1801, 869], [1716, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkPMdW9l_pHs",
        "outputId": "225ea5e4-cd9c-48cd-82af-7bb718ff6097"
      },
      "source": [
        "len(annotations)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhInlCf07y-3"
      },
      "source": [
        "import re\r\n",
        "import ast\r\n",
        "import numpy as np\r\n",
        "def str2array(s):\r\n",
        "    # Remove space after [\r\n",
        "    s=re.sub('\\[ +', '[', s.strip())\r\n",
        "    # Replace commas and spaces\r\n",
        "    s=re.sub('[,\\s]+', ', ', s)\r\n",
        "    return np.array(ast.literal_eval(s))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltu_ndY1Kgkk",
        "outputId": "d1010b5b-1a0c-4426-fb97-018ffadd917d"
      },
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "\r\n",
        "msk = np.random.rand(len(annotations)) < 0.4\r\n",
        "train_samples = annotations[msk]\r\n",
        "validation_samples = annotations[~msk]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "sample = annotations.iloc[1, :]\r\n",
        "landmarks = sample['data']\r\n",
        "print(landmarks)\r\n",
        "landmarks = np.array(str2array(landmarks))\r\n",
        "print(type(landmarks))\r\n",
        "# landmarks = np.array(list(landmarks))\r\n",
        "print(landmarks.shape)\r\n",
        "print(landmarks)\r\n",
        "\r\n",
        "# print('Image name: {}'.format(img_name))\r\n",
        "# print('Landmarks shape: {}'.format(landmarks.shape))\r\n",
        "# print('First 4 Landmarks: {}'.format(landmarks[:4]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1328, 7], [1347, 101], [1383, 193], [1400, 267], [1411, 366], [1400, 428], [1387, 545], [1394, 640], [1400, 707], [1417, 783], [1432, 852], [1462, 953], [1457, 1006]]\n",
            "<class 'numpy.ndarray'>\n",
            "(13, 2)\n",
            "[[1328    7]\n",
            " [1347  101]\n",
            " [1383  193]\n",
            " [1400  267]\n",
            " [1411  366]\n",
            " [1400  428]\n",
            " [1387  545]\n",
            " [1394  640]\n",
            " [1400  707]\n",
            " [1417  783]\n",
            " [1432  852]\n",
            " [1462  953]\n",
            " [1457 1006]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDO0eWSppIjD"
      },
      "source": [
        "from PIL import Image\r\n",
        "import random\r\n",
        "import torch.nn.functional as F\r\n",
        "from math import cos, sin, radians\r\n",
        "import imutils\r\n",
        "import cv2\r\n",
        "\r\n",
        "\r\n",
        "class RANZCRDataset():\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, csv_file='/content/trainset/train.csv', root_dir='/content/trainset/data/1', transform=None, images_name=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.Images_name = images_name\r\n",
        "        self.root_dir = root_dir\r\n",
        "        self.transform = transform\r\n",
        "        \r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.Images_name)\r\n",
        "\r\n",
        "    #############\r\n",
        "\r\n",
        "\r\n",
        "    # def get_rot_mat(self, theta):\r\n",
        "\r\n",
        "    #   theta = torch.tensor(theta)\r\n",
        "    #   return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\r\n",
        "    #                         [torch.sin(theta), torch.cos(theta), 0]])\r\n",
        "\r\n",
        "\r\n",
        "    # def rot_img_landmark(self, x, landmarks, theta, dtype):\r\n",
        "    #     rot_mat = self.get_rot_mat(theta)[None, ...].type(dtype).repeat(x.shape[0],1,1)\r\n",
        "    #     grid = F.affine_grid(rot_mat, x.size()).type(dtype)\r\n",
        "    #     image = F.grid_sample(x, grid)\r\n",
        "    #     landmarks = landmarks - 0.5\r\n",
        "    #     new_landmarks = np.matmul(landmarks, transformation_matrix)\r\n",
        "    #     new_landmarks = new_landmarks + 0.5\r\n",
        "    #     return image, new_landmarks\r\n",
        "\r\n",
        "\r\n",
        "    ##############\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        img_name = os.path.join(self.root_dir,\r\n",
        "                                self.Images_name.iloc[idx, 0])\r\n",
        "        image = Image.open(img_name + '.jpg').convert('RGB')\r\n",
        "        labels = self.Images_name.iloc[idx, -1]\r\n",
        "        labels = torch.from_numpy(str2array(labels))\r\n",
        "        sample = {'image': image, 'label': labels }\r\n",
        "\r\n",
        "\r\n",
        "        if self.transform:\r\n",
        "\r\n",
        "          \r\n",
        "          tmp = np.zeros((68, 2))\r\n",
        "\r\n",
        "          for i in range(sample['label'].shape[0]):\r\n",
        "\r\n",
        "            tmp[i, 0] = (centerCrop_value / np.array(image).shape[1]) * np.array(sample['label'])[i, 0]\r\n",
        "            tmp[i, 1] = (centerCrop_value / np.array(image).shape[0]) * np.array(sample['label'])[i, 1]\r\n",
        "             \r\n",
        "\r\n",
        "          sample['label'] = torch.from_numpy(tmp).type(torch.float16)\r\n",
        "\r\n",
        "\r\n",
        "          sample['image'] = self.transform(sample['image'])\r\n",
        "\r\n",
        "\r\n",
        "          # # random rotation\r\n",
        "          # image, landmark = self.rot_img_landmark(sample['image'], sample['label'], np.pi/2, dtype= torch.FloatTensor)\r\n",
        "          # print(type(image), image.shape)\r\n",
        "          # print(type(landmark), landmark.shape)\r\n",
        "          \r\n",
        "\r\n",
        "        return sample\r\n",
        "\r\n",
        "# my_dataset = RANZCRDataset\r\n",
        "# my_dataset.__getitem__(self, 4)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojvstcXDbmN"
      },
      "source": [
        "\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchvision import transforms, utils\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import os\r\n",
        "\r\n",
        "batch_size = 8\r\n",
        "mean = np.array([0.4823, 0.4823, 0.4823])\r\n",
        "std = np.array([0.191473164, 0.191473164, 0.191473164])\r\n",
        "\r\n",
        "centerCrop_value = 244\r\n",
        "transform = transforms.Compose([transforms.ToTensor(),\r\n",
        "                                transforms.Resize((256, 256)),\r\n",
        "                                transforms.CenterCrop(centerCrop_value),\r\n",
        "                                transforms.Normalize(mean, std)])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "trainset = RANZCRDataset(csv_file='/content/trainset/annotations/train_annotations.csv',\r\n",
        "                                    root_dir='/content/trainset/data/1', transform=transform, images_name=train_samples)\r\n",
        "\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(trainset,\r\n",
        "                         batch_size=batch_size,\r\n",
        "                         num_workers=0,\r\n",
        "                         shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "validation_set = RANZCRDataset(csv_file='/content/trainset/annotations/train_annotations.csv',\r\n",
        "                                    root_dir='/content/trainset/data/1', transform=transform, images_name=validation_samples)\r\n",
        "\r\n",
        "\r\n",
        "validation_loader = torch.utils.data.DataLoader(trainset,\r\n",
        "                         batch_size=batch_size,\r\n",
        "                         num_workers=0,\r\n",
        "                         shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "cHNoNytaOALb",
        "outputId": "8dc9faa8-4bf1-4af0-bf39-5a5a8f6b3e6a"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "def imshow(img, landmarks):\r\n",
        "    npimg = img.numpy()\r\n",
        "    npimg = ((npimg * std[0]) + mean[0]) # unnormalize\r\n",
        "    plt.imshow((np.transpose(npimg, (1, 2, 0)) * 255).astype(np.uint8))\r\n",
        "    for i in range(landmarks.shape[0]):\r\n",
        "      landmarks[i, :, 0] = landmarks[i, :, 0] + (centerCrop_value*i)\r\n",
        "    plt.scatter(landmarks[:, :, 0], landmarks[:, :, 1], s=10, marker='.', c='r')\r\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "# get some random training images\r\n",
        "dataiter = iter(train_loader)\r\n",
        "sample = dataiter.next()\r\n",
        "\r\n",
        "imshow(torchvision.utils.make_grid(sample['image']), sample['label'])\r\n",
        "print(sample['label'].shape)\r\n",
        "print(sample['image'].shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 68, 2])\n",
            "torch.Size([8, 3, 244, 244])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05o0AD8DzDyf"
      },
      "source": [
        "# class Network(nn.Module):\r\n",
        "  \r\n",
        "#   def __init__(self):\r\n",
        "#     super(Network, self).__init__()\r\n",
        "#     self.model = model\r\n",
        "#     self.conv1 = nn.Conv2d(3, 3, 5)\r\n",
        "#     self.conv2 = nn.Conv2d(3, 3, 1)\r\n",
        "#     self.pool2 = nn.MaxPool2d(2, 2)\r\n",
        "#     self.sigmoid = nn.Sigmoid()\r\n",
        "#     self.fc_final = nn.Linear(1000, 11)\r\n",
        "\r\n",
        "#   def forward(self, x):\r\n",
        "\r\n",
        "#     x = self.pool2(F.relu(self.conv1(x)))\r\n",
        "#     x = self.pool2(F.relu(self.conv2(x)))\r\n",
        "#     x = self.model(x)\r\n",
        "#     x = self.sigmoid(self.fc_final(x))\r\n",
        "#     return x\r\n",
        "\r\n",
        "# Network = Network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdKfqyGawGY8"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "from torchvision import models\r\n",
        "from torch import optim\r\n",
        "import time\r\n",
        "\r\n",
        "model = models.resnet152(pretrained=False)\r\n",
        "\r\n",
        "class Network(nn.Module):\r\n",
        "    def __init__(self,num_classes=136):\r\n",
        "        super().__init__()\r\n",
        "        self.model_name='resnet18'\r\n",
        "        self.model=model\r\n",
        "        self.fc_final = nn.Linear(1000, 136)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.model(x)\r\n",
        "        x = F.relu(self.fc_final(x))\r\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVUdllE-2WHE"
      },
      "source": [
        "Network = Network()\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = optim.Adam(Network.parameters(), lr=0.0075, weight_decay=1e-06)\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BscbT2lMMMHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d71e1e-f072-4103-fcf7-9ed19421eede"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print(count_parameters(model))\r\n",
        "print(count_parameters(Network))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60192808\n",
            "60328944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yOUc5MLn2H8B",
        "outputId": "4381a489-babb-435e-ae94-f9c4a26cc9b5"
      },
      "source": [
        "\r\n",
        "for epoch in range(40):\r\n",
        "\r\n",
        "  running_loss = 0.0\r\n",
        "  best_validation_loss = 1000000000000\r\n",
        "\r\n",
        "  for batch_number, data in enumerate(train_loader, 0):\r\n",
        "    \r\n",
        "  \r\n",
        "    inputs = data['image'].float()\r\n",
        "    label_temp = data['label']\r\n",
        "    label = np.zeros((batch_size, 136))\r\n",
        "    for i in range(batch_size):\r\n",
        "      if label_temp.shape[0] == batch_size:\r\n",
        "        label[i, :68] = label_temp[i, :, 0]\r\n",
        "        label[i, 68:] = label_temp[i, :, 1]\r\n",
        "      else:\r\n",
        "        pass\r\n",
        "\r\n",
        "    label = torch.from_numpy(label).float()\r\n",
        "    optimizer.zero_grad()\r\n",
        "    outputs = Network(inputs)\r\n",
        "    loss = criterion(outputs, label)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    running_loss += loss.item()\r\n",
        "    if (batch_number % 10 == 0) and (batch_number > 9):\r\n",
        "      print('[epoch: %d, batch: %5d] training loss: %.3f' %( epoch + 1, batch_number, (running_loss/10)))\r\n",
        "      \r\n",
        "      dataiter = iter(validation_loader)\r\n",
        "      sample = dataiter.next()\r\n",
        "      validation_inputs = sample['image'].float()\r\n",
        "      validation_label_temp = data['label']\r\n",
        "      validation_label = np.zeros((batch_size, 136))\r\n",
        "      for i in range(batch_size):\r\n",
        "        if validation_label_temp.shape[0] == batch_size:\r\n",
        "          validation_label[i, :68] = validation_label_temp[i, :, 0]\r\n",
        "          validation_label[i, 68:] = validation_label_temp[i, :, 1]\r\n",
        "        else:\r\n",
        "          pass\r\n",
        "\r\n",
        "      validation_label = torch.from_numpy(validation_label).float()\r\n",
        "      validation_outputs = Network(validation_inputs)\r\n",
        "      validation_loss = criterion(validation_outputs, validation_label)\r\n",
        "      \r\n",
        "\r\n",
        "      print('[epoch: %d, batch: %5d] <validation 10 random sample> loss: %.3f' %( epoch + 1, batch_number, validation_loss))\r\n",
        "\r\n",
        "\r\n",
        "      running_loss = 0.0\r\n",
        "\r\n",
        "  try:\r\n",
        "    if validation_loss < best_validation_loss:\r\n",
        "      torch.save(Network.state_dict(), '/content/drive/MyDrive/landmarks.pth')\r\n",
        "      best_validation_loss = validation_loss\r\n",
        "  except ValueError:\r\n",
        "    pass\r\n",
        "  \r\n",
        "    \r\n",
        "print('Finished Training Network')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch: 1, batch:    10] training loss: 1570.635\n",
            "[epoch: 1, batch:    10] <validation 10 random sample> loss: 843.638\n",
            "[epoch: 1, batch:    20] training loss: 1057.810\n",
            "[epoch: 1, batch:    20] <validation 10 random sample> loss: 1391.703\n",
            "[epoch: 1, batch:    30] training loss: 960.650\n",
            "[epoch: 1, batch:    30] <validation 10 random sample> loss: 652.755\n",
            "[epoch: 1, batch:    40] training loss: 1222.799\n",
            "[epoch: 1, batch:    40] <validation 10 random sample> loss: 949.858\n",
            "[epoch: 1, batch:    50] training loss: 938.923\n",
            "[epoch: 1, batch:    50] <validation 10 random sample> loss: 509.950\n",
            "[epoch: 1, batch:    60] training loss: 1030.899\n",
            "[epoch: 1, batch:    60] <validation 10 random sample> loss: 762.595\n",
            "[epoch: 1, batch:    70] training loss: 1215.795\n",
            "[epoch: 1, batch:    70] <validation 10 random sample> loss: 1417.153\n",
            "[epoch: 1, batch:    80] training loss: 893.683\n",
            "[epoch: 1, batch:    80] <validation 10 random sample> loss: 1039.480\n",
            "[epoch: 1, batch:    90] training loss: 1018.521\n",
            "[epoch: 1, batch:    90] <validation 10 random sample> loss: 649.577\n",
            "[epoch: 1, batch:   100] training loss: 1074.565\n",
            "[epoch: 1, batch:   100] <validation 10 random sample> loss: 891.194\n",
            "[epoch: 1, batch:   110] training loss: 1119.393\n",
            "[epoch: 1, batch:   110] <validation 10 random sample> loss: 1316.683\n",
            "[epoch: 1, batch:   120] training loss: 950.886\n",
            "[epoch: 1, batch:   120] <validation 10 random sample> loss: 606.376\n",
            "[epoch: 1, batch:   130] training loss: 927.284\n",
            "[epoch: 1, batch:   130] <validation 10 random sample> loss: 843.970\n",
            "[epoch: 1, batch:   140] training loss: 993.847\n",
            "[epoch: 1, batch:   140] <validation 10 random sample> loss: 614.966\n",
            "[epoch: 1, batch:   150] training loss: 985.102\n",
            "[epoch: 1, batch:   150] <validation 10 random sample> loss: 1088.235\n",
            "[epoch: 1, batch:   160] training loss: 760.670\n",
            "[epoch: 1, batch:   160] <validation 10 random sample> loss: 1573.438\n",
            "[epoch: 1, batch:   170] training loss: 992.551\n",
            "[epoch: 1, batch:   170] <validation 10 random sample> loss: 835.367\n",
            "[epoch: 1, batch:   180] training loss: 868.540\n",
            "[epoch: 1, batch:   180] <validation 10 random sample> loss: 857.880\n",
            "[epoch: 1, batch:   190] training loss: 1217.459\n",
            "[epoch: 1, batch:   190] <validation 10 random sample> loss: 983.821\n",
            "[epoch: 1, batch:   200] training loss: 776.305\n",
            "[epoch: 1, batch:   200] <validation 10 random sample> loss: 815.152\n",
            "[epoch: 1, batch:   210] training loss: 826.282\n",
            "[epoch: 1, batch:   210] <validation 10 random sample> loss: 1381.476\n",
            "[epoch: 1, batch:   220] training loss: 1066.733\n",
            "[epoch: 1, batch:   220] <validation 10 random sample> loss: 932.474\n",
            "[epoch: 1, batch:   230] training loss: 1093.573\n",
            "[epoch: 1, batch:   230] <validation 10 random sample> loss: 1333.949\n",
            "[epoch: 1, batch:   240] training loss: 1258.374\n",
            "[epoch: 1, batch:   240] <validation 10 random sample> loss: 1097.282\n",
            "[epoch: 1, batch:   250] training loss: 1196.604\n",
            "[epoch: 1, batch:   250] <validation 10 random sample> loss: 1564.378\n",
            "[epoch: 1, batch:   260] training loss: 1144.250\n",
            "[epoch: 1, batch:   260] <validation 10 random sample> loss: 550.169\n",
            "[epoch: 1, batch:   270] training loss: 1079.524\n",
            "[epoch: 1, batch:   270] <validation 10 random sample> loss: 941.126\n",
            "[epoch: 1, batch:   280] training loss: 1220.965\n",
            "[epoch: 1, batch:   280] <validation 10 random sample> loss: 1083.836\n",
            "[epoch: 1, batch:   290] training loss: 1068.488\n",
            "[epoch: 1, batch:   290] <validation 10 random sample> loss: 901.171\n",
            "[epoch: 1, batch:   300] training loss: 1102.636\n",
            "[epoch: 1, batch:   300] <validation 10 random sample> loss: 488.279\n",
            "[epoch: 1, batch:   310] training loss: 920.370\n",
            "[epoch: 1, batch:   310] <validation 10 random sample> loss: 739.994\n",
            "[epoch: 1, batch:   320] training loss: 893.570\n",
            "[epoch: 1, batch:   320] <validation 10 random sample> loss: 887.417\n",
            "[epoch: 1, batch:   330] training loss: 1100.153\n",
            "[epoch: 1, batch:   330] <validation 10 random sample> loss: 941.371\n",
            "[epoch: 1, batch:   340] training loss: 990.339\n",
            "[epoch: 1, batch:   340] <validation 10 random sample> loss: 585.276\n",
            "[epoch: 1, batch:   350] training loss: 1187.670\n",
            "[epoch: 1, batch:   350] <validation 10 random sample> loss: 688.545\n",
            "[epoch: 1, batch:   360] training loss: 944.107\n",
            "[epoch: 1, batch:   360] <validation 10 random sample> loss: 1339.556\n",
            "[epoch: 1, batch:   370] training loss: 1090.649\n",
            "[epoch: 1, batch:   370] <validation 10 random sample> loss: 1655.709\n",
            "[epoch: 1, batch:   380] training loss: 954.594\n",
            "[epoch: 1, batch:   380] <validation 10 random sample> loss: 1745.225\n",
            "[epoch: 1, batch:   390] training loss: 950.777\n",
            "[epoch: 1, batch:   390] <validation 10 random sample> loss: 987.713\n",
            "[epoch: 1, batch:   400] training loss: 984.316\n",
            "[epoch: 1, batch:   400] <validation 10 random sample> loss: 1007.875\n",
            "[epoch: 1, batch:   410] training loss: 1079.439\n",
            "[epoch: 1, batch:   410] <validation 10 random sample> loss: 876.856\n",
            "[epoch: 1, batch:   420] training loss: 1278.245\n",
            "[epoch: 1, batch:   420] <validation 10 random sample> loss: 882.205\n",
            "[epoch: 1, batch:   430] training loss: 1197.446\n",
            "[epoch: 1, batch:   430] <validation 10 random sample> loss: 1785.441\n",
            "[epoch: 1, batch:   440] training loss: 786.887\n",
            "[epoch: 1, batch:   440] <validation 10 random sample> loss: 784.214\n",
            "[epoch: 1, batch:   450] training loss: 950.064\n",
            "[epoch: 1, batch:   450] <validation 10 random sample> loss: 1199.849\n",
            "[epoch: 1, batch:   460] training loss: 938.605\n",
            "[epoch: 1, batch:   460] <validation 10 random sample> loss: 621.195\n",
            "[epoch: 1, batch:   470] training loss: 1154.447\n",
            "[epoch: 1, batch:   470] <validation 10 random sample> loss: 2238.375\n",
            "[epoch: 1, batch:   480] training loss: 939.721\n",
            "[epoch: 1, batch:   480] <validation 10 random sample> loss: 1188.595\n",
            "[epoch: 1, batch:   490] training loss: 1223.073\n",
            "[epoch: 1, batch:   490] <validation 10 random sample> loss: 471.210\n",
            "[epoch: 1, batch:   500] training loss: 968.992\n",
            "[epoch: 1, batch:   500] <validation 10 random sample> loss: 853.569\n",
            "[epoch: 1, batch:   510] training loss: 968.441\n",
            "[epoch: 1, batch:   510] <validation 10 random sample> loss: 589.009\n",
            "[epoch: 1, batch:   520] training loss: 1092.329\n",
            "[epoch: 1, batch:   520] <validation 10 random sample> loss: 765.904\n",
            "[epoch: 1, batch:   530] training loss: 936.838\n",
            "[epoch: 1, batch:   530] <validation 10 random sample> loss: 1119.644\n",
            "[epoch: 1, batch:   540] training loss: 1260.648\n",
            "[epoch: 1, batch:   540] <validation 10 random sample> loss: 913.887\n",
            "[epoch: 1, batch:   550] training loss: 1011.549\n",
            "[epoch: 1, batch:   550] <validation 10 random sample> loss: 793.393\n",
            "[epoch: 1, batch:   560] training loss: 1039.654\n",
            "[epoch: 1, batch:   560] <validation 10 random sample> loss: 1804.051\n",
            "[epoch: 1, batch:   570] training loss: 960.960\n",
            "[epoch: 1, batch:   570] <validation 10 random sample> loss: 1123.133\n",
            "[epoch: 1, batch:   580] training loss: 986.620\n",
            "[epoch: 1, batch:   580] <validation 10 random sample> loss: 1484.518\n",
            "[epoch: 1, batch:   590] training loss: 1254.847\n",
            "[epoch: 1, batch:   590] <validation 10 random sample> loss: 1705.770\n",
            "[epoch: 1, batch:   600] training loss: 1067.251\n",
            "[epoch: 1, batch:   600] <validation 10 random sample> loss: 940.811\n",
            "[epoch: 1, batch:   610] training loss: 1003.809\n",
            "[epoch: 1, batch:   610] <validation 10 random sample> loss: 1084.249\n",
            "[epoch: 1, batch:   620] training loss: 1050.724\n",
            "[epoch: 1, batch:   620] <validation 10 random sample> loss: 1466.218\n",
            "[epoch: 1, batch:   630] training loss: 1147.609\n",
            "[epoch: 1, batch:   630] <validation 10 random sample> loss: 1138.635\n",
            "[epoch: 1, batch:   640] training loss: 1020.582\n",
            "[epoch: 1, batch:   640] <validation 10 random sample> loss: 1766.898\n",
            "[epoch: 1, batch:   650] training loss: 948.254\n",
            "[epoch: 1, batch:   650] <validation 10 random sample> loss: 950.143\n",
            "[epoch: 1, batch:   660] training loss: 967.823\n",
            "[epoch: 1, batch:   660] <validation 10 random sample> loss: 1626.906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([8, 200])) that is different to the input size (torch.Size([7, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3ef34479cce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (8) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "fGd8r4eTwbJW",
        "outputId": "b977dc52-e9d8-40b3-dfda-afa0f5cf040a"
      },
      "source": [
        "# network = Network()\r\n",
        "# network.cuda()    \r\n",
        "\r\n",
        "# criterion = nn.MSELoss()\r\n",
        "# optimizer = optim.Adam(network.parameters(), lr=0.0001)\r\n",
        "\r\n",
        "# loss_min = np.inf\r\n",
        "# num_epochs = 10\r\n",
        "\r\n",
        "# start_time = time.time()\r\n",
        "# for epoch in range(1,num_epochs+1):\r\n",
        "    \r\n",
        "#     loss_train = 0\r\n",
        "#     loss_valid = 0\r\n",
        "#     running_loss = 0\r\n",
        "    \r\n",
        "#     network.train()\r\n",
        "#     for step in range(1,len(loader)+1):\r\n",
        "\r\n",
        "#         sample = next(iter(loader))\r\n",
        "#         images, landmarks = sample['image'], sample['label']\r\n",
        "        \r\n",
        "#         images = images.cuda()\r\n",
        "#         landmarks = landmarks.view(landmarks.size(0),-1).cuda() \r\n",
        "        \r\n",
        "#         predictions = network(images)\r\n",
        "        \r\n",
        "#         # clear all the gradients before calculating them\r\n",
        "#         optimizer.zero_grad()\r\n",
        "        \r\n",
        "#         # find the loss for the current step\r\n",
        "#         loss_train_step = criterion(predictions, landmarks)\r\n",
        "        \r\n",
        "#         # calculate the gradients\r\n",
        "#         loss_train_step.backward()\r\n",
        "        \r\n",
        "#         # update the parameters\r\n",
        "#         optimizer.step()\r\n",
        "        \r\n",
        "#         loss_train += loss_train_step.item()\r\n",
        "#         running_loss = loss_train/step\r\n",
        "        \r\n",
        "#         # print_overwrite(step, len(train_loader), running_loss, 'train')\r\n",
        "        \r\n",
        "#     network.eval() \r\n",
        "#     with torch.no_grad():\r\n",
        "        \r\n",
        "#         for step in range(1,  3): #len(loader)+1):\r\n",
        "            \r\n",
        "#             images, landmarks = next(iter(loader))\r\n",
        "        \r\n",
        "#             images = images.cuda()\r\n",
        "#             landmarks = landmarks.view(landmarks.size(0),-1).cuda()\r\n",
        "        \r\n",
        "#             predictions = network(images)\r\n",
        "\r\n",
        "#             # find the loss for the current step\r\n",
        "#             loss_valid_step = criterion(predictions, landmarks)\r\n",
        "\r\n",
        "#             loss_valid += loss_valid_step.item()\r\n",
        "#             running_loss = loss_valid/step\r\n",
        "\r\n",
        "#             print_overwrite(step, len(valid_loader), running_loss, 'valid')\r\n",
        "    \r\n",
        "#     loss_train /= len(train_loader)\r\n",
        "#     loss_valid /= len(valid_loader)\r\n",
        "    \r\n",
        "#     print('\\n--------------------------------------------------')\r\n",
        "#     print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\r\n",
        "#     print('--------------------------------------------------')\r\n",
        "    \r\n",
        "#     if loss_valid < loss_min:\r\n",
        "#         loss_min = loss_valid\r\n",
        "#         torch.save(network.state_dict(), '/content/face_landmarks.pth') \r\n",
        "#         print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\r\n",
        "#         print('Model Saved\\n')\r\n",
        "     \r\n",
        "# print('Training Complete')\r\n",
        "# print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-372-1d4fbc5f89c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-367-4dadf7cc0547>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenterCrop_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenterCrop_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJIX-wbgMO1C"
      },
      "source": [
        "# find dataset mean and std\r\n",
        "\r\n",
        "# import torch\r\n",
        "# from torch import Tensor\r\n",
        "# from typing import Iterable\r\n",
        "# from fastprogress import progress_bar\r\n",
        "\r\n",
        "# class RunningStatistics:\r\n",
        "#     '''Records mean and variance of the final `n_dims` dimension over other dimensions across items. So collecting across `(l,m,n,o)` sized\r\n",
        "#        items with `n_dims=1` will collect `(l,m,n)` sized statistics while with `n_dims=2` the collected statistics will be of size `(l,m)`.\r\n",
        "#        Uses the algorithm from Chan, Golub, and LeVeque in \"Algorithms for computing the sample variance: analysis and recommendations\":\r\n",
        "#        `variance = variance1 + variance2 + n/(m*(m+n)) * pow(((m/n)*t1 - t2), 2)`\r\n",
        "#        This combines the variance for 2 blocks: block 1 having `n` elements with `variance1` and a sum of `t1` and block 2 having `m` elements\r\n",
        "#        with `variance2` and a sum of `t2`. The algorithm is proven to be numerically stable but there is a reasonable loss of accuracy (~0.1% error).\r\n",
        "#        Note that collecting minimum and maximum values is reasonably innefficient, adding about 80% to the running time, and hence is disabled by default.\r\n",
        "#     '''\r\n",
        "#     def __init__(self, n_dims:int=2, record_range=False):\r\n",
        "#         self._n_dims,self._range = n_dims,record_range\r\n",
        "#         self.n,self.sum,self.min,self.max = 0,None,None,None\r\n",
        "    \r\n",
        "#     def update(self, data:Tensor):\r\n",
        "\r\n",
        "#         data = data.view(*list(data.shape[:-self._n_dims]) + [-1])\r\n",
        "#         with torch.no_grad():\r\n",
        "#             new_n,new_var,new_sum = data.shape[-1],data.var(-1),data.sum(-1)\r\n",
        "#             if self.n == 0:\r\n",
        "#                 self.n = new_n\r\n",
        "#                 self._shape = data.shape[:-1]\r\n",
        "#                 self.sum = new_sum\r\n",
        "#                 self._nvar = new_var.mul_(new_n)\r\n",
        "#                 if self._range:\r\n",
        "#                     self.min = data.min(-1)[0]\r\n",
        "#                     self.max = data.max(-1)[0]\r\n",
        "#             else:\r\n",
        "#                 # assert data.shape[:-1] == self._shape, f\"Mismatched shapes, expected {self._shape} but got {data.shape[:-1]}.\"\r\n",
        "#                 ratio = self.n / new_n\r\n",
        "#                 t = (self.sum / ratio).sub_(new_sum).pow_(2)\r\n",
        "#                 self._nvar.add_(new_n, new_var).add_(ratio / (self.n + new_n), t)\r\n",
        "#                 self.sum.add_(new_sum)\r\n",
        "#                 self.n += new_n\r\n",
        "#                 if self._range:\r\n",
        "#                     self.min = torch.min(self.min, data.min(-1)[0])\r\n",
        "#                     self.max = torch.max(self.max, data.max(-1)[0])\r\n",
        "\r\n",
        "#     @property\r\n",
        "#     def mean(self): return self.sum / self.n if self.n > 0 else None\r\n",
        "#     @property\r\n",
        "#     def var(self): return self._nvar / self.n if self.n > 0 else None\r\n",
        "#     @property\r\n",
        "#     def std(self): return self.var.sqrt() if self.n > 0 else None\r\n",
        "\r\n",
        "#     def __repr__(self):\r\n",
        "#         def _fmt_t(t:Tensor):\r\n",
        "#             if t.numel() > 5: return f\"tensor of ({','.join(map(str,t.shape))})\"\r\n",
        "#             def __fmt_t(t:Tensor):\r\n",
        "#                 return '[' + ','.join([f\"{v:.3g}\" if v.ndim==0 else __fmt_t(v) for v in t]) + ']'\r\n",
        "#             return __fmt_t(t)\r\n",
        "#         rng_str = f\", min={_fmt_t(self.min)}, max={_fmt_t(self.max)}\" if self._range else \"\"\r\n",
        "#         return f\"RunningStatistics(n={self.n}, mean={_fmt_t(self.mean)}, std={_fmt_t(self.std)}{rng_str})\"\r\n",
        "\r\n",
        "# def collect_stats(items:Iterable, n_dims:int=2, record_range:bool=False):\r\n",
        "#     stats = RunningStatistics(n_dims, record_range)\r\n",
        "#     for it in progress_bar(items.next()):\r\n",
        "#         it = it.float()\r\n",
        "#         if hasattr(it, 'data'):\r\n",
        "#             stats.update(it.data)\r\n",
        "#         else:\r\n",
        "#             stats.update(it)\r\n",
        "#     return stats\r\n",
        "\r\n",
        "# dd = RunningStatistics\r\n",
        "# stats = collect_stats(dataiter)\r\n",
        "# stats\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZXTf5UJDsbO",
        "outputId": "37e138e7-9b65-4653-d0f3-ab3d22356726"
      },
      "source": [
        "# stats.mean.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5057, 0.5057, 0.5057])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGmx2VtMJKQi",
        "outputId": "be0872a5-6dcd-43c5-e056-bc4732399513"
      },
      "source": [
        "# stats.std.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1902, 0.1902, 0.1902])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOCxit_OJ787"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}