{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Annotations.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "365b2fc0cb5947948a8e8ef2c30def1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a4c605dd1e284eb29afa5634ac5c7488",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f9725415bf0c47e7a4915829cf76c915",
              "IPY_MODEL_763ed3d8453942feb9315223ffee67fe"
            ]
          }
        },
        "a4c605dd1e284eb29afa5634ac5c7488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9725415bf0c47e7a4915829cf76c915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fdad636789f8410d8a338da601e1fd79",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a042db30047e4c12b92d46ad68cf6ae0"
          }
        },
        "763ed3d8453942feb9315223ffee67fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae0608d8c0f04e4889d5e2867050cfe9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:48&lt;00:00, 2.11MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25d366673a544bb39cbc493eb0c12d79"
          }
        },
        "fdad636789f8410d8a338da601e1fd79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a042db30047e4c12b92d46ad68cf6ae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae0608d8c0f04e4889d5e2867050cfe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25d366673a544bb39cbc493eb0c12d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezash97/Machine-Learning-Course/blob/main/Final_Project/Annotations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvHWw5FNjHCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a1792c-745d-42ff-faf4-47cf1fb53fff"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzvA1piVH8Ok"
      },
      "source": [
        "# !wget 'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/23870/1781260/compressed/train.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1612694077&Signature=NSRoUkKvZLhO%2BLEKz%2BSSD9UX2MbYAdeDeolxa%2FdRIaRRlGcsX4SbKOzbMsqojrlAut2mizJQMJoWDUcwvq5kI4nFdlWyOzpmuPBOfWOblp%2BkW3whOXreh7v0Sm7RGGhe3dR193%2FWMABkA0tHP%2BgjOBTyBWSKgjhaSEIiIURY%2FLrB0nYcqINyTi3vaFrpy7mlDQvQYFYfufjwKETiC92JQ0R6S5Q5WPjGV6k47qUhO7FbZhHqyAVATYRBjv7jbGtnyTdiBgtzegxcQ0GP362RYVVkVNHxgbo6EBpVMMuiTE%2F9qmWom%2FoiNDdlP8O8GPRGJNZNm3PrdGKHLOfQl9yvSg%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.zip'\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# !wget 'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/23870/1781260/compressed/train_annotations.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1612694119&Signature=Czs91HbT9fo5CsoshJ0tdUahQX6o4Q0MdJ4x4PB%2FmrKhOtjpg7VUtLylMqsmvduEpTzFTDFAOKuAdphKX%2Fh2y5AzaeJ2HYgRaUyc%2F27pSlCdDwEm48uglNhgqKadHh4OW3b19BRyjFZDxBcVydYHnYowDQgqG3EYdqrAEwsdMnQ8yfJWbwd2FLT6RYPSYW5Jv67yOB3K%2FOiFTvshKisM8LNRIq%2BFZn%2BjfoJKJxM1DPMZquMceWsFAqTuo66uAgFNuoaZ%2FbjLqkBVnqeSlhS%2BVzv3678VNHsF6O%2BWl3LyQQJf6swTlTI%2F8ZcUF7U%2BAqZsqWN5eRbVpfjXY%2FCXuMz2Ag%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain_annotations.csv.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE0z02ObIO_J"
      },
      "source": [
        "# !mkdir /content/trainset\r\n",
        "# !mkdir /content/trainset/data/\r\n",
        "# !mkdir /content/trainset/data/1/\r\n",
        "# !unzip '/content/train.zip' -d /content/trainset/data/1/\r\n",
        "# !unzip '/content/train_annotations.csv.zip' -d /content/trainset/annotations/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "4N1FWlaEJTw8",
        "outputId": "5170cf4b-d74e-4fed-d3a6-7056ff80786b"
      },
      "source": [
        "import pandas as pd \r\n",
        "annotations = pd.read_csv('/content/trainset/annotations/train_annotations.csv')\r\n",
        "annotations.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>label</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12616281126973421762...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1487, 1279], [1477, 1168], [1472, 1052], [14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12616281126973421762...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1328, 7], [1347, 101], [1383, 193], [1400, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.72921907356394389969...</td>\n",
              "      <td>CVC - Borderline</td>\n",
              "      <td>[[801, 1207], [812, 1112], [823, 1023], [842, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.11697104485452001927...</td>\n",
              "      <td>CVC - Normal</td>\n",
              "      <td>[[1366, 961], [1411, 861], [1453, 751], [1508,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.87704688663091069148...</td>\n",
              "      <td>NGT - Normal</td>\n",
              "      <td>[[1862, 14], [1845, 293], [1801, 869], [1716, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    StudyInstanceUID  ...                                               data\n",
              "0  1.2.826.0.1.3680043.8.498.12616281126973421762...  ...  [[1487, 1279], [1477, 1168], [1472, 1052], [14...\n",
              "1  1.2.826.0.1.3680043.8.498.12616281126973421762...  ...  [[1328, 7], [1347, 101], [1383, 193], [1400, 2...\n",
              "2  1.2.826.0.1.3680043.8.498.72921907356394389969...  ...  [[801, 1207], [812, 1112], [823, 1023], [842, ...\n",
              "3  1.2.826.0.1.3680043.8.498.11697104485452001927...  ...  [[1366, 961], [1411, 861], [1453, 751], [1508,...\n",
              "4  1.2.826.0.1.3680043.8.498.87704688663091069148...  ...  [[1862, 14], [1845, 293], [1801, 869], [1716, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkPMdW9l_pHs",
        "outputId": "771d3e74-ad7b-437c-ed13-d475c2341174"
      },
      "source": [
        "len(annotations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhInlCf07y-3"
      },
      "source": [
        "import re\r\n",
        "import ast\r\n",
        "import numpy as np\r\n",
        "def str2array(s):\r\n",
        "    # Remove space after [\r\n",
        "    s=re.sub('\\[ +', '[', s.strip())\r\n",
        "    # Replace commas and spaces\r\n",
        "    s=re.sub('[,\\s]+', ', ', s)\r\n",
        "    return np.array(ast.literal_eval(s))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltu_ndY1Kgkk",
        "outputId": "2ada7e47-2285-4306-af07-ff6a7d61b6ea"
      },
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "##############\r\n",
        "msk_for_dataset_subset = np.random.rand(len(annotations)) < 6.00\r\n",
        "dataset_subset = annotations[msk_for_dataset_subset]\r\n",
        "##############\r\n",
        "\r\n",
        "msk = np.random.rand(len(dataset_subset)) > 0.25\r\n",
        "train_samples = dataset_subset[msk]\r\n",
        "validation_samples = dataset_subset[~msk]\r\n",
        "\r\n",
        "##############\r\n",
        "train_samples = train_samples[:5000]\r\n",
        "validation_samples = validation_samples[:1500]\r\n",
        "##############\r\n",
        "print('number of train samples: ', len(train_samples))\r\n",
        "print('number of validation samples: ', len(validation_samples))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "sample = dataset_subset.iloc[1, :]\r\n",
        "landmarks = sample['data']\r\n",
        "# print(landmarks)\r\n",
        "landmarks = np.array(str2array(landmarks))\r\n",
        "# print(type(landmarks))\r\n",
        "# landmarks = np.array(list(landmarks))\r\n",
        "print(\"sample landmark shape: \", landmarks.shape)\r\n",
        "# print(landmarks)\r\n",
        "\r\n",
        "# print('Image name: {}'.format(img_name))\r\n",
        "# print('Landmarks shape: {}'.format(landmarks.shape))\r\n",
        "# print('First 4 Landmarks: {}'.format(landmarks[:4]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of train samples:  5000\n",
            "number of validation samples:  1500\n",
            "sample landmark shape:  (13, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDO0eWSppIjD"
      },
      "source": [
        "from PIL import Image\r\n",
        "import random\r\n",
        "import torch.nn.functional as F\r\n",
        "from math import cos, sin, radians\r\n",
        "import imutils\r\n",
        "import cv2\r\n",
        "\r\n",
        "\r\n",
        "class RANZCRDataset():\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, csv_file='/content/trainset/train.csv', root_dir='/content/trainset/data/1', transform=None, images_name=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.Images_name = images_name\r\n",
        "        self.root_dir = root_dir\r\n",
        "        self.transform = transform\r\n",
        "        \r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.Images_name)\r\n",
        "\r\n",
        "    #############\r\n",
        "\r\n",
        "\r\n",
        "    # def get_rot_mat(self, theta):\r\n",
        "\r\n",
        "    #   theta = torch.tensor(theta)\r\n",
        "    #   return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\r\n",
        "    #                         [torch.sin(theta), torch.cos(theta), 0]])\r\n",
        "\r\n",
        "\r\n",
        "    # def rot_img_landmark(self, x, landmarks, theta, dtype):\r\n",
        "    #     rot_mat = self.get_rot_mat(theta)[None, ...].type(dtype).repeat(x.shape[0],1,1)\r\n",
        "    #     grid = F.affine_grid(rot_mat, x.size()).type(dtype)\r\n",
        "    #     image = F.grid_sample(x, grid)\r\n",
        "    #     landmarks = landmarks - 0.5\r\n",
        "    #     new_landmarks = np.matmul(landmarks, transformation_matrix)\r\n",
        "    #     new_landmarks = new_landmarks + 0.5\r\n",
        "    #     return image, new_landmarks\r\n",
        "\r\n",
        "\r\n",
        "    ##############\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        centerCrop_value = 904\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        img_name = os.path.join(self.root_dir,\r\n",
        "                                self.Images_name.iloc[idx, 0])\r\n",
        "        image = Image.open(img_name + '.jpg').convert('RGB')\r\n",
        "        labels = self.Images_name.iloc[idx, -1]\r\n",
        "        labels = torch.from_numpy(str2array(labels))\r\n",
        "        sample = {'image': image, 'label': labels }\r\n",
        "\r\n",
        "\r\n",
        "        if self.transform:\r\n",
        "\r\n",
        "          \r\n",
        "          tmp = np.zeros((100, 2))\r\n",
        "\r\n",
        "          for i in range(0, (sample['label'].shape[0]) ):\r\n",
        "            tmp[i, 0] = ( (centerCrop_value / np.array(image).shape[1]) * np.array(sample['label'])[i, 0] ) / centerCrop_value\r\n",
        "            tmp[i, 1] = ( (centerCrop_value / np.array(image).shape[0]) * np.array(sample['label'])[i, 1] ) / centerCrop_value\r\n",
        "             \r\n",
        "\r\n",
        "          sample['label'] = torch.from_numpy(tmp).type(torch.float16)\r\n",
        "\r\n",
        "\r\n",
        "          sample['image'] = self.transform(sample['image'])\r\n",
        "\r\n",
        "\r\n",
        "          # # random rotation\r\n",
        "          # image, landmark = self.rot_img_landmark(sample['image'], sample['label'], np.pi/2, dtype= torch.FloatTensor)\r\n",
        "          # print(type(image), image.shape)\r\n",
        "          # print(type(landmark), landmark.shape)\r\n",
        "          \r\n",
        "\r\n",
        "        return sample\r\n",
        "\r\n",
        "# my_dataset = RANZCRDataset\r\n",
        "# my_dataset.__getitem__(self, 4)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojvstcXDbmN"
      },
      "source": [
        "\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchvision import transforms, utils\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import os\r\n",
        "\r\n",
        "# batch_size = 8\r\n",
        "mean = np.array([0.4823, 0.4823, 0.4823])\r\n",
        "std = np.array([0.191473164, 0.191473164, 0.191473164])\r\n",
        "\r\n",
        "\r\n",
        "def load_data(csv_file='/content/trainset/annotations/train_annotations.csv', root_dir='/content/trainset/data/1'):\r\n",
        "\r\n",
        "  centerCrop_value = 904\r\n",
        "  transform = transforms.Compose([transforms.ToTensor(),\r\n",
        "                                  transforms.Resize((1024, 1024)),\r\n",
        "                                  transforms.CenterCrop(centerCrop_value),\r\n",
        "                                  transforms.Normalize(mean, std)])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  trainset = RANZCRDataset(csv_file='/content/trainset/annotations/train_annotations.csv',\r\n",
        "                                      root_dir='/content/trainset/data/1', transform=transform, images_name=train_samples)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  validation_set = RANZCRDataset(csv_file='/content/trainset/annotations/train_annotations.csv',\r\n",
        "                                    root_dir='/content/trainset/data/1', transform=transform, images_name=validation_samples)\r\n",
        "  \r\n",
        "\r\n",
        "  return trainset, validation_set\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L2-Z934TG4k"
      },
      "source": [
        "\r\n",
        "# train_loader = torch.utils.data.DataLoader(trainset,\r\n",
        "#                          batch_size=batch_size,\r\n",
        "#                          num_workers=0,\r\n",
        "#                          shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# validation_loader = torch.utils.data.DataLoader(trainset,\r\n",
        "#                          batch_size=batch_size,\r\n",
        "#                          num_workers=0,\r\n",
        "#                          shuffle=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHNoNytaOALb"
      },
      "source": [
        "# import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "# def imshow(img, landmarks):\r\n",
        "#     npimg = img.numpy()\r\n",
        "#     npimg = ((npimg * std[0]) + mean[0]) # unnormalize\r\n",
        "#     plt.imshow((np.transpose(npimg, (1, 2, 0)) * 255).astype(np.uint8))\r\n",
        "#     for i in range(landmarks.shape[0]):\r\n",
        "#       landmarks[i, :, 0] = landmarks[i, :, 0] + (centerCrop_value*i)\r\n",
        "#     plt.scatter(landmarks[:, :, 0], landmarks[:, :, 1], s=10, marker='.', c='r')\r\n",
        "#     plt.pause(0.001)  # pause a bit so that plots are updated\r\n",
        "#     plt.show()\r\n",
        "\r\n",
        "\r\n",
        "# # get some random training images\r\n",
        "# dataiter = iter(train_loader)\r\n",
        "# sample = dataiter.next()\r\n",
        "\r\n",
        "# imshow(torchvision.utils.make_grid(sample['image']), sample['label'])\r\n",
        "# print(sample['label'].shape)\r\n",
        "# print(sample['image'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05o0AD8DzDyf"
      },
      "source": [
        "# class Network(nn.Module):\r\n",
        "  \r\n",
        "#   def __init__(self):\r\n",
        "#     super(Network, self).__init__()\r\n",
        "#     self.model = model\r\n",
        "#     self.conv1 = nn.Conv2d(3, 3, 5)\r\n",
        "#     self.conv2 = nn.Conv2d(3, 3, 1)\r\n",
        "#     self.pool2 = nn.MaxPool2d(2, 2)\r\n",
        "#     self.sigmoid = nn.Sigmoid()\r\n",
        "#     self.fc_final = nn.Linear(1000, 11)\r\n",
        "\r\n",
        "#   def forward(self, x):\r\n",
        "\r\n",
        "#     x = self.pool2(F.relu(self.conv1(x)))\r\n",
        "#     x = self.pool2(F.relu(self.conv2(x)))\r\n",
        "#     x = self.model(x)\r\n",
        "#     x = self.sigmoid(self.fc_final(x))\r\n",
        "#     return x\r\n",
        "\r\n",
        "# Network = Network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdKfqyGawGY8",
        "outputId": "f6793c49-e4b8-491e-fadb-5bea4f7a1dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "365b2fc0cb5947948a8e8ef2c30def1e",
            "a4c605dd1e284eb29afa5634ac5c7488",
            "f9725415bf0c47e7a4915829cf76c915",
            "763ed3d8453942feb9315223ffee67fe",
            "fdad636789f8410d8a338da601e1fd79",
            "a042db30047e4c12b92d46ad68cf6ae0",
            "ae0608d8c0f04e4889d5e2867050cfe9",
            "25d366673a544bb39cbc493eb0c12d79"
          ]
        }
      },
      "source": [
        "import torch.nn as nn\r\n",
        "from torchvision import models\r\n",
        "from torch import optim\r\n",
        "import time\r\n",
        "\r\n",
        "model = models.resnet50(pretrained=True)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "365b2fc0cb5947948a8e8ef2c30def1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9nsdk53Ufm2"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F \r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self, l1=512, l2=256, c1=3):\r\n",
        "    super(Net, self).__init__()\r\n",
        "    self.model = model\r\n",
        "    self.conv1 = nn.Conv2d(3, c1, 5)\r\n",
        "    self.conv2 = nn.Conv2d(c1, 3, 1)\r\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\r\n",
        "    self.sigmoid = nn.Sigmoid()\r\n",
        "    self.fc1 = nn.Linear(1000, l1)\r\n",
        "    self.fc2 = nn.Linear(l1, l2)\r\n",
        "    self.fc_final = nn.Linear(l2, 200)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "\r\n",
        "    x = self.pool2(F.relu(self.conv1(x)))\r\n",
        "    x = self.pool2(F.relu(self.conv2(x)))\r\n",
        "    x = self.model(x)\r\n",
        "    x = F.relu(self.fc1(x))\r\n",
        "    x = F.relu(self.fc2(x))\r\n",
        "    x = self.sigmoid((self.fc_final(x)))\r\n",
        "    return x\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAhsF_T4UxTv"
      },
      "source": [
        "# ###################################\r\n",
        "# !pip install ray\r\n",
        "# !pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5tHqN1MUqo_"
      },
      "source": [
        "from functools import partial\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import random_split\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from ray import tune\r\n",
        "from ray.tune import CLIReporter\r\n",
        "from ray.tune.schedulers import ASHAScheduler\r\n",
        "import tensorboardX"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugZO6cnRUvF2"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "def r2_score(y_targets, y_preds):\r\n",
        "\r\n",
        "    \r\n",
        "    try:\r\n",
        "        from sklearn.metrics import r2_score\r\n",
        "    except ImportError:\r\n",
        "        raise RuntimeError(\"This contrib module requires sklearn to be installed.\")\r\n",
        "\r\n",
        "\r\n",
        "    y_true = y_targets.cpu().detach().numpy().astype('float16')\r\n",
        "    y_pred = y_preds.cpu().detach().numpy().astype('float16')\r\n",
        "    try: \r\n",
        "      return r2_score(y_true, y_pred)\r\n",
        "    except ValueError:\r\n",
        "      return 0\r\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyc_At_1VJ2j"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "def train_ranzcr_landmark(config, checkpoint_dir=None, data_dir=None):\r\n",
        "    net = Net(config[\"l1\"], config[\"l2\"])\r\n",
        "\r\n",
        "    device = \"cpu\"\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        device = \"cuda:0\"\r\n",
        "        if torch.cuda.device_count() > 1:\r\n",
        "            net = nn.DataParallel(net)\r\n",
        "    net.to(device)\r\n",
        "\r\n",
        "    criterion = nn.MSELoss()\r\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], weight_decay=config[\"wd\"])\r\n",
        "\r\n",
        "    if checkpoint_dir:\r\n",
        "        model_state, optimizer_state = torch.load(\r\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\r\n",
        "        net.load_state_dict(model_state)\r\n",
        "        optimizer.load_state_dict(optimizer_state)\r\n",
        "\r\n",
        "    trainset, testset = load_data(data_dir)\r\n",
        "\r\n",
        "    test_abs = int(len(trainset) * 0.8)\r\n",
        "    train_subset, val_subset = random_split(\r\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\r\n",
        "    \r\n",
        "    trainloader = torch.utils.data.DataLoader(\r\n",
        "        train_subset,\r\n",
        "        batch_size=int(config[\"batch_size\"]),\r\n",
        "        shuffle=True,\r\n",
        "        num_workers=8)\r\n",
        "    valloader = torch.utils.data.DataLoader(\r\n",
        "        val_subset,\r\n",
        "        batch_size=int(config[\"batch_size\"]),\r\n",
        "        shuffle=True,\r\n",
        "        num_workers=8)\r\n",
        "\r\n",
        "    for epoch in range(15):  # loop over the dataset multiple times\r\n",
        "        running_loss = 0.0\r\n",
        "        epoch_steps = 0\r\n",
        "        for i, data in enumerate(trainloader, 0):\r\n",
        "            # get the inputs; data is a list of [inputs, labels]\r\n",
        "            inputs, labels_temp = data['image'].float(), data['label']\r\n",
        "            batch_size = int(inputs.shape[0])\r\n",
        "            labels = np.zeros((batch_size, 200))\r\n",
        "            for i in range(batch_size):\r\n",
        "              labels[i, :100] = labels_temp[i, :, 0]\r\n",
        "              labels[i, 100:] = labels_temp[i, :, 1]\r\n",
        "\r\n",
        "            labels = (torch.from_numpy(labels)).type(torch.float16)\r\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "            # zero the parameter gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            # forward + backward + optimize\r\n",
        "            outputs = (net(inputs)).type(torch.float16)\r\n",
        "            loss = criterion(outputs, labels)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            # print statistics\r\n",
        "            running_loss += loss.item()\r\n",
        "            epoch_steps += 1\r\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\r\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\r\n",
        "                                                running_loss / epoch_steps))\r\n",
        "                running_loss = 0.0\r\n",
        "\r\n",
        "        # Validation loss\r\n",
        "        val_loss = 0.0\r\n",
        "        val_steps = 0\r\n",
        "        total = 0\r\n",
        "        correct = 0\r\n",
        "        accuracy = 0\r\n",
        "        batch_counter = 0\r\n",
        "        for i, data in enumerate(valloader, 0):\r\n",
        "            with torch.no_grad():\r\n",
        "                inputs, labels_temp = data['image'].float(), data['label']\r\n",
        "                batch_size = int(inputs.shape[0])\r\n",
        "                labels = np.zeros((batch_size, 200))\r\n",
        "                for i in range(batch_size):\r\n",
        "                  labels[i, :100] = labels_temp[i, :, 0]\r\n",
        "                  labels[i, 100:] = labels_temp[i, :, 1]\r\n",
        "\r\n",
        "                labels = (torch.from_numpy(labels)).type(torch.float16)\r\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "                outputs = (net(inputs)).type(torch.float16)\r\n",
        "                # _, predicted = torch.max(outputs.data, 1)\r\n",
        "                # total += labels.size(0)\r\n",
        "                # correct += (predicted == labels).sum().item()\r\n",
        "                # my_validation_outputs = (outputs > 0.5)\r\n",
        "                # print('------------------------------------------------------------------------------')\r\n",
        "                # print(labels)\r\n",
        "                # print(my_validation_outputs)\r\n",
        "                # print('------------------------------------------------------------------------------')\r\n",
        "                # accuracy += auc_s(labels, my_validation_outputs)\r\n",
        "                # batch_counter += 1\r\n",
        "                accuracy += r2_score(labels, outputs)\r\n",
        "                batch_counter += 1\r\n",
        "\r\n",
        "                loss = criterion(outputs, labels)\r\n",
        "                val_loss += loss.cpu().numpy()\r\n",
        "                val_steps += 1\r\n",
        "\r\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\r\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\r\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\r\n",
        "\r\n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=(accuracy / batch_counter))\r\n",
        "    torch.save((net.state_dict(), optimizer.state_dict()), '/content/drive/MyDrive/RANZCR/model_landmark_%.3f.pth'%(accuracy / batch_counter))\r\n",
        "    print(\"Finished Training\")\r\n",
        "\r\n",
        "\r\n",
        "            "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRwirW14ZY3g"
      },
      "source": [
        "\r\n",
        "def test_accuracy(net, device=\"cpu\"):\r\n",
        "    trainset, testset = load_data()\r\n",
        "\r\n",
        "    testloader = torch.utils.data.DataLoader(\r\n",
        "        testset, batch_size=4, shuffle=False, num_workers=2)\r\n",
        "\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    batch_counter_test = 0\r\n",
        "    accuracy_test = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data in testloader:\r\n",
        "            images, labels = data['image'].float(), data['label']\r\n",
        "            batch_size = int(inputs.shape[0])\r\n",
        "            labels = np.zeros((batch_size, 200))\r\n",
        "            for i in range(batch_size):\r\n",
        "              labels[i, :100] = labels_temp[i, :, 0]\r\n",
        "              labels[i, 100:] = labels_temp[i, :, 1]\r\n",
        "\r\n",
        "            labels = (torch.from_numpy(labels)).type(torch.float16)\r\n",
        "            images, labels = images.to(device), labels.to(device)\r\n",
        "            outputs = (net(images)).type(torch.float16)\r\n",
        "            # _, predicted = torch.max(outputs.data, 1)\r\n",
        "            # total += labels.size(0)\r\n",
        "            # correct += (predicted == labels).sum().item()\r\n",
        "            # my_test_outputs = (outputs > 0.5)\r\n",
        "            # accuracy_test += accuracy_score(labels, my_test_outputs)\r\n",
        "            # batch_counter_test += 1\r\n",
        "            accuracy_test += r2_score(labels, outputs)\r\n",
        "            batch_counter_test += 1\r\n",
        "\r\n",
        "\r\n",
        "    return accuracy_test / batch_counter_test"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aCE8hHqZzri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500b1ce9-0a82-4411-f3ca-f7efe32f3461"
      },
      "source": [
        "\r\n",
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\r\n",
        "    data_dir = os.path.abspath(\"/content/trainset/data\")\r\n",
        "    load_data(data_dir)\r\n",
        "    config = {\r\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(8, 9)),\r\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(8, 9)),\r\n",
        "        \"lr\": tune.loguniform(1e-3, 1e-1),\r\n",
        "        \"wd\": tune.loguniform(1e-6, 1e-5),\r\n",
        "        \"c1\": tune.sample_from(lambda _: 2 ** np.random.randint(5, 6)),\r\n",
        "        \"batch_size\": tune.choice([8])\r\n",
        "    }\r\n",
        "    scheduler = ASHAScheduler(\r\n",
        "        metric=\"loss\",\r\n",
        "        mode=\"min\",\r\n",
        "        max_t=max_num_epochs,\r\n",
        "        grace_period=1,\r\n",
        "        reduction_factor=2)\r\n",
        "    reporter = CLIReporter(\r\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\r\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\r\n",
        "\r\n",
        "    result = tune.run(\r\n",
        "            partial(train_ranzcr_landmark, data_dir=data_dir),\r\n",
        "            resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\r\n",
        "            config=config,\r\n",
        "            num_samples=num_samples,\r\n",
        "            scheduler=scheduler,\r\n",
        "            progress_reporter=reporter)\r\n",
        "      \r\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\r\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\r\n",
        "    print(\"Best trial final validation loss: {}\".format(\r\n",
        "        best_trial.last_result[\"loss\"]))\r\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\r\n",
        "        best_trial.last_result[\"accuracy\"]))\r\n",
        "\r\n",
        "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\r\n",
        "    device = \"cpu\"\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        device = \"cuda:0\"\r\n",
        "        if gpus_per_trial > 1:\r\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\r\n",
        "    best_trained_model.to(device)\r\n",
        "\r\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\r\n",
        "    model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\r\n",
        "    best_trained_model.load_state_dict(model_state)\r\n",
        "    test_acc = test_accuracy(best_trained_model, device)\r\n",
        "    torch.save(best_trained_model.state_dict(), '/content/drive/MyDrive/RANZCR/model_landmark_%.3f.pth'%(test_acc) )\r\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    # You can change the number of GPUs per trial here:\r\n",
        "    main(num_samples=1, max_num_epochs=20, gpus_per_trial=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-04 11:12:15,482\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
            "2021-02-04 11:12:19,663\tWARNING experiment.py:285 -- No name detected on trainable. Using DEFAULT.\n",
            "2021-02-04 11:12:19,672\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n",
            "2021-02-04 11:12:24,209\tWARNING worker.py:1034 -- Warning: The actor ImplicitFunc has size 104545223 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n",
            "2021-02-04 11:12:24,360\tWARNING util.py:142 -- The `start_trial` operation took 2.428 s, which may be a performance bottleneck.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 1.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.52 GiB heap, 0.0/2.59 GiB objects (0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-02-04_11-12-20\n",
            "Number of trials: 1/1 (1 RUNNING)\n",
            "+---------------------+----------+-------+--------------+------+------+------+-----------+-------------+\n",
            "| Trial name          | status   | loc   |   batch_size |   c1 |   l1 |   l2 |        lr |          wd |\n",
            "|---------------------+----------+-------+--------------+------+------+------+-----------+-------------|\n",
            "| DEFAULT_d9a2e_00000 | RUNNING  |       |            8 |   32 |  256 |  256 | 0.0183939 | 6.36659e-06 |\n",
            "+---------------------+----------+-------+--------------+------+------+------+-----------+-------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVUdllE-2WHE"
      },
      "source": [
        "\r\n",
        "#########################################################\r\n",
        "Network = Network()\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = optim.Adam(Network.parameters(), lr=0.0075, weight_decay=1e-06)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BscbT2lMMMHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d71e1e-f072-4103-fcf7-9ed19421eede"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print(count_parameters(model))\r\n",
        "print(count_parameters(Network))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60192808\n",
            "60328944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yOUc5MLn2H8B",
        "outputId": "4381a489-babb-435e-ae94-f9c4a26cc9b5"
      },
      "source": [
        "\r\n",
        "for epoch in range(40):\r\n",
        "\r\n",
        "  running_loss = 0.0\r\n",
        "  best_validation_loss = 1000000000000\r\n",
        "\r\n",
        "  for batch_number, data in enumerate(train_loader, 0):\r\n",
        "    \r\n",
        "  \r\n",
        "    inputs = data['image'].float()\r\n",
        "    label_temp = data['label']\r\n",
        "    label = np.zeros((batch_size, 136))\r\n",
        "    for i in range(batch_size):\r\n",
        "      if label_temp.shape[0] == batch_size:\r\n",
        "        label[i, :68] = label_temp[i, :, 0]\r\n",
        "        label[i, 68:] = label_temp[i, :, 1]\r\n",
        "      else:\r\n",
        "        pass\r\n",
        "\r\n",
        "    label = torch.from_numpy(label).float()\r\n",
        "    optimizer.zero_grad()\r\n",
        "    outputs = Network(inputs)\r\n",
        "    loss = criterion(outputs, label)\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    running_loss += loss.item()\r\n",
        "    if (batch_number % 10 == 0) and (batch_number > 9):\r\n",
        "      print('[epoch: %d, batch: %5d] training loss: %.3f' %( epoch + 1, batch_number, (running_loss/10)))\r\n",
        "      \r\n",
        "      dataiter = iter(validation_loader)\r\n",
        "      sample = dataiter.next()\r\n",
        "      validation_inputs = sample['image'].float()\r\n",
        "      validation_label_temp = data['label']\r\n",
        "      validation_label = np.zeros((batch_size, 136))\r\n",
        "      for i in range(batch_size):\r\n",
        "        if validation_label_temp.shape[0] == batch_size:\r\n",
        "          validation_label[i, :68] = validation_label_temp[i, :, 0]\r\n",
        "          validation_label[i, 68:] = validation_label_temp[i, :, 1]\r\n",
        "        else:\r\n",
        "          pass\r\n",
        "\r\n",
        "      validation_label = torch.from_numpy(validation_label).float()\r\n",
        "      validation_outputs = Network(validation_inputs)\r\n",
        "      validation_loss = criterion(validation_outputs, validation_label)\r\n",
        "      \r\n",
        "\r\n",
        "      print('[epoch: %d, batch: %5d] <validation 10 random sample> loss: %.3f' %( epoch + 1, batch_number, validation_loss))\r\n",
        "\r\n",
        "\r\n",
        "      running_loss = 0.0\r\n",
        "\r\n",
        "  try:\r\n",
        "    if validation_loss < best_validation_loss:\r\n",
        "      torch.save(Network.state_dict(), '/content/drive/MyDrive/landmarks.pth')\r\n",
        "      best_validation_loss = validation_loss\r\n",
        "  except ValueError:\r\n",
        "    pass\r\n",
        "  \r\n",
        "    \r\n",
        "print('Finished Training Network')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch: 1, batch:    10] training loss: 1570.635\n",
            "[epoch: 1, batch:    10] <validation 10 random sample> loss: 843.638\n",
            "[epoch: 1, batch:    20] training loss: 1057.810\n",
            "[epoch: 1, batch:    20] <validation 10 random sample> loss: 1391.703\n",
            "[epoch: 1, batch:    30] training loss: 960.650\n",
            "[epoch: 1, batch:    30] <validation 10 random sample> loss: 652.755\n",
            "[epoch: 1, batch:    40] training loss: 1222.799\n",
            "[epoch: 1, batch:    40] <validation 10 random sample> loss: 949.858\n",
            "[epoch: 1, batch:    50] training loss: 938.923\n",
            "[epoch: 1, batch:    50] <validation 10 random sample> loss: 509.950\n",
            "[epoch: 1, batch:    60] training loss: 1030.899\n",
            "[epoch: 1, batch:    60] <validation 10 random sample> loss: 762.595\n",
            "[epoch: 1, batch:    70] training loss: 1215.795\n",
            "[epoch: 1, batch:    70] <validation 10 random sample> loss: 1417.153\n",
            "[epoch: 1, batch:    80] training loss: 893.683\n",
            "[epoch: 1, batch:    80] <validation 10 random sample> loss: 1039.480\n",
            "[epoch: 1, batch:    90] training loss: 1018.521\n",
            "[epoch: 1, batch:    90] <validation 10 random sample> loss: 649.577\n",
            "[epoch: 1, batch:   100] training loss: 1074.565\n",
            "[epoch: 1, batch:   100] <validation 10 random sample> loss: 891.194\n",
            "[epoch: 1, batch:   110] training loss: 1119.393\n",
            "[epoch: 1, batch:   110] <validation 10 random sample> loss: 1316.683\n",
            "[epoch: 1, batch:   120] training loss: 950.886\n",
            "[epoch: 1, batch:   120] <validation 10 random sample> loss: 606.376\n",
            "[epoch: 1, batch:   130] training loss: 927.284\n",
            "[epoch: 1, batch:   130] <validation 10 random sample> loss: 843.970\n",
            "[epoch: 1, batch:   140] training loss: 993.847\n",
            "[epoch: 1, batch:   140] <validation 10 random sample> loss: 614.966\n",
            "[epoch: 1, batch:   150] training loss: 985.102\n",
            "[epoch: 1, batch:   150] <validation 10 random sample> loss: 1088.235\n",
            "[epoch: 1, batch:   160] training loss: 760.670\n",
            "[epoch: 1, batch:   160] <validation 10 random sample> loss: 1573.438\n",
            "[epoch: 1, batch:   170] training loss: 992.551\n",
            "[epoch: 1, batch:   170] <validation 10 random sample> loss: 835.367\n",
            "[epoch: 1, batch:   180] training loss: 868.540\n",
            "[epoch: 1, batch:   180] <validation 10 random sample> loss: 857.880\n",
            "[epoch: 1, batch:   190] training loss: 1217.459\n",
            "[epoch: 1, batch:   190] <validation 10 random sample> loss: 983.821\n",
            "[epoch: 1, batch:   200] training loss: 776.305\n",
            "[epoch: 1, batch:   200] <validation 10 random sample> loss: 815.152\n",
            "[epoch: 1, batch:   210] training loss: 826.282\n",
            "[epoch: 1, batch:   210] <validation 10 random sample> loss: 1381.476\n",
            "[epoch: 1, batch:   220] training loss: 1066.733\n",
            "[epoch: 1, batch:   220] <validation 10 random sample> loss: 932.474\n",
            "[epoch: 1, batch:   230] training loss: 1093.573\n",
            "[epoch: 1, batch:   230] <validation 10 random sample> loss: 1333.949\n",
            "[epoch: 1, batch:   240] training loss: 1258.374\n",
            "[epoch: 1, batch:   240] <validation 10 random sample> loss: 1097.282\n",
            "[epoch: 1, batch:   250] training loss: 1196.604\n",
            "[epoch: 1, batch:   250] <validation 10 random sample> loss: 1564.378\n",
            "[epoch: 1, batch:   260] training loss: 1144.250\n",
            "[epoch: 1, batch:   260] <validation 10 random sample> loss: 550.169\n",
            "[epoch: 1, batch:   270] training loss: 1079.524\n",
            "[epoch: 1, batch:   270] <validation 10 random sample> loss: 941.126\n",
            "[epoch: 1, batch:   280] training loss: 1220.965\n",
            "[epoch: 1, batch:   280] <validation 10 random sample> loss: 1083.836\n",
            "[epoch: 1, batch:   290] training loss: 1068.488\n",
            "[epoch: 1, batch:   290] <validation 10 random sample> loss: 901.171\n",
            "[epoch: 1, batch:   300] training loss: 1102.636\n",
            "[epoch: 1, batch:   300] <validation 10 random sample> loss: 488.279\n",
            "[epoch: 1, batch:   310] training loss: 920.370\n",
            "[epoch: 1, batch:   310] <validation 10 random sample> loss: 739.994\n",
            "[epoch: 1, batch:   320] training loss: 893.570\n",
            "[epoch: 1, batch:   320] <validation 10 random sample> loss: 887.417\n",
            "[epoch: 1, batch:   330] training loss: 1100.153\n",
            "[epoch: 1, batch:   330] <validation 10 random sample> loss: 941.371\n",
            "[epoch: 1, batch:   340] training loss: 990.339\n",
            "[epoch: 1, batch:   340] <validation 10 random sample> loss: 585.276\n",
            "[epoch: 1, batch:   350] training loss: 1187.670\n",
            "[epoch: 1, batch:   350] <validation 10 random sample> loss: 688.545\n",
            "[epoch: 1, batch:   360] training loss: 944.107\n",
            "[epoch: 1, batch:   360] <validation 10 random sample> loss: 1339.556\n",
            "[epoch: 1, batch:   370] training loss: 1090.649\n",
            "[epoch: 1, batch:   370] <validation 10 random sample> loss: 1655.709\n",
            "[epoch: 1, batch:   380] training loss: 954.594\n",
            "[epoch: 1, batch:   380] <validation 10 random sample> loss: 1745.225\n",
            "[epoch: 1, batch:   390] training loss: 950.777\n",
            "[epoch: 1, batch:   390] <validation 10 random sample> loss: 987.713\n",
            "[epoch: 1, batch:   400] training loss: 984.316\n",
            "[epoch: 1, batch:   400] <validation 10 random sample> loss: 1007.875\n",
            "[epoch: 1, batch:   410] training loss: 1079.439\n",
            "[epoch: 1, batch:   410] <validation 10 random sample> loss: 876.856\n",
            "[epoch: 1, batch:   420] training loss: 1278.245\n",
            "[epoch: 1, batch:   420] <validation 10 random sample> loss: 882.205\n",
            "[epoch: 1, batch:   430] training loss: 1197.446\n",
            "[epoch: 1, batch:   430] <validation 10 random sample> loss: 1785.441\n",
            "[epoch: 1, batch:   440] training loss: 786.887\n",
            "[epoch: 1, batch:   440] <validation 10 random sample> loss: 784.214\n",
            "[epoch: 1, batch:   450] training loss: 950.064\n",
            "[epoch: 1, batch:   450] <validation 10 random sample> loss: 1199.849\n",
            "[epoch: 1, batch:   460] training loss: 938.605\n",
            "[epoch: 1, batch:   460] <validation 10 random sample> loss: 621.195\n",
            "[epoch: 1, batch:   470] training loss: 1154.447\n",
            "[epoch: 1, batch:   470] <validation 10 random sample> loss: 2238.375\n",
            "[epoch: 1, batch:   480] training loss: 939.721\n",
            "[epoch: 1, batch:   480] <validation 10 random sample> loss: 1188.595\n",
            "[epoch: 1, batch:   490] training loss: 1223.073\n",
            "[epoch: 1, batch:   490] <validation 10 random sample> loss: 471.210\n",
            "[epoch: 1, batch:   500] training loss: 968.992\n",
            "[epoch: 1, batch:   500] <validation 10 random sample> loss: 853.569\n",
            "[epoch: 1, batch:   510] training loss: 968.441\n",
            "[epoch: 1, batch:   510] <validation 10 random sample> loss: 589.009\n",
            "[epoch: 1, batch:   520] training loss: 1092.329\n",
            "[epoch: 1, batch:   520] <validation 10 random sample> loss: 765.904\n",
            "[epoch: 1, batch:   530] training loss: 936.838\n",
            "[epoch: 1, batch:   530] <validation 10 random sample> loss: 1119.644\n",
            "[epoch: 1, batch:   540] training loss: 1260.648\n",
            "[epoch: 1, batch:   540] <validation 10 random sample> loss: 913.887\n",
            "[epoch: 1, batch:   550] training loss: 1011.549\n",
            "[epoch: 1, batch:   550] <validation 10 random sample> loss: 793.393\n",
            "[epoch: 1, batch:   560] training loss: 1039.654\n",
            "[epoch: 1, batch:   560] <validation 10 random sample> loss: 1804.051\n",
            "[epoch: 1, batch:   570] training loss: 960.960\n",
            "[epoch: 1, batch:   570] <validation 10 random sample> loss: 1123.133\n",
            "[epoch: 1, batch:   580] training loss: 986.620\n",
            "[epoch: 1, batch:   580] <validation 10 random sample> loss: 1484.518\n",
            "[epoch: 1, batch:   590] training loss: 1254.847\n",
            "[epoch: 1, batch:   590] <validation 10 random sample> loss: 1705.770\n",
            "[epoch: 1, batch:   600] training loss: 1067.251\n",
            "[epoch: 1, batch:   600] <validation 10 random sample> loss: 940.811\n",
            "[epoch: 1, batch:   610] training loss: 1003.809\n",
            "[epoch: 1, batch:   610] <validation 10 random sample> loss: 1084.249\n",
            "[epoch: 1, batch:   620] training loss: 1050.724\n",
            "[epoch: 1, batch:   620] <validation 10 random sample> loss: 1466.218\n",
            "[epoch: 1, batch:   630] training loss: 1147.609\n",
            "[epoch: 1, batch:   630] <validation 10 random sample> loss: 1138.635\n",
            "[epoch: 1, batch:   640] training loss: 1020.582\n",
            "[epoch: 1, batch:   640] <validation 10 random sample> loss: 1766.898\n",
            "[epoch: 1, batch:   650] training loss: 948.254\n",
            "[epoch: 1, batch:   650] <validation 10 random sample> loss: 950.143\n",
            "[epoch: 1, batch:   660] training loss: 967.823\n",
            "[epoch: 1, batch:   660] <validation 10 random sample> loss: 1626.906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([8, 200])) that is different to the input size (torch.Size([7, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3ef34479cce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (8) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "fGd8r4eTwbJW",
        "outputId": "b977dc52-e9d8-40b3-dfda-afa0f5cf040a"
      },
      "source": [
        "# network = Network()\r\n",
        "# network.cuda()    \r\n",
        "\r\n",
        "# criterion = nn.MSELoss()\r\n",
        "# optimizer = optim.Adam(network.parameters(), lr=0.0001)\r\n",
        "\r\n",
        "# loss_min = np.inf\r\n",
        "# num_epochs = 10\r\n",
        "\r\n",
        "# start_time = time.time()\r\n",
        "# for epoch in range(1,num_epochs+1):\r\n",
        "    \r\n",
        "#     loss_train = 0\r\n",
        "#     loss_valid = 0\r\n",
        "#     running_loss = 0\r\n",
        "    \r\n",
        "#     network.train()\r\n",
        "#     for step in range(1,len(loader)+1):\r\n",
        "\r\n",
        "#         sample = next(iter(loader))\r\n",
        "#         images, landmarks = sample['image'], sample['label']\r\n",
        "        \r\n",
        "#         images = images.cuda()\r\n",
        "#         landmarks = landmarks.view(landmarks.size(0),-1).cuda() \r\n",
        "        \r\n",
        "#         predictions = network(images)\r\n",
        "        \r\n",
        "#         # clear all the gradients before calculating them\r\n",
        "#         optimizer.zero_grad()\r\n",
        "        \r\n",
        "#         # find the loss for the current step\r\n",
        "#         loss_train_step = criterion(predictions, landmarks)\r\n",
        "        \r\n",
        "#         # calculate the gradients\r\n",
        "#         loss_train_step.backward()\r\n",
        "        \r\n",
        "#         # update the parameters\r\n",
        "#         optimizer.step()\r\n",
        "        \r\n",
        "#         loss_train += loss_train_step.item()\r\n",
        "#         running_loss = loss_train/step\r\n",
        "        \r\n",
        "#         # print_overwrite(step, len(train_loader), running_loss, 'train')\r\n",
        "        \r\n",
        "#     network.eval() \r\n",
        "#     with torch.no_grad():\r\n",
        "        \r\n",
        "#         for step in range(1,  3): #len(loader)+1):\r\n",
        "            \r\n",
        "#             images, landmarks = next(iter(loader))\r\n",
        "        \r\n",
        "#             images = images.cuda()\r\n",
        "#             landmarks = landmarks.view(landmarks.size(0),-1).cuda()\r\n",
        "        \r\n",
        "#             predictions = network(images)\r\n",
        "\r\n",
        "#             # find the loss for the current step\r\n",
        "#             loss_valid_step = criterion(predictions, landmarks)\r\n",
        "\r\n",
        "#             loss_valid += loss_valid_step.item()\r\n",
        "#             running_loss = loss_valid/step\r\n",
        "\r\n",
        "#             print_overwrite(step, len(valid_loader), running_loss, 'valid')\r\n",
        "    \r\n",
        "#     loss_train /= len(train_loader)\r\n",
        "#     loss_valid /= len(valid_loader)\r\n",
        "    \r\n",
        "#     print('\\n--------------------------------------------------')\r\n",
        "#     print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\r\n",
        "#     print('--------------------------------------------------')\r\n",
        "    \r\n",
        "#     if loss_valid < loss_min:\r\n",
        "#         loss_min = loss_valid\r\n",
        "#         torch.save(network.state_dict(), '/content/face_landmarks.pth') \r\n",
        "#         print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\r\n",
        "#         print('Model Saved\\n')\r\n",
        "     \r\n",
        "# print('Training Complete')\r\n",
        "# print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-372-1d4fbc5f89c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-367-4dadf7cc0547>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenterCrop_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenterCrop_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJIX-wbgMO1C"
      },
      "source": [
        "# find dataset mean and std\r\n",
        "\r\n",
        "# import torch\r\n",
        "# from torch import Tensor\r\n",
        "# from typing import Iterable\r\n",
        "# from fastprogress import progress_bar\r\n",
        "\r\n",
        "# class RunningStatistics:\r\n",
        "#     '''Records mean and variance of the final `n_dims` dimension over other dimensions across items. So collecting across `(l,m,n,o)` sized\r\n",
        "#        items with `n_dims=1` will collect `(l,m,n)` sized statistics while with `n_dims=2` the collected statistics will be of size `(l,m)`.\r\n",
        "#        Uses the algorithm from Chan, Golub, and LeVeque in \"Algorithms for computing the sample variance: analysis and recommendations\":\r\n",
        "#        `variance = variance1 + variance2 + n/(m*(m+n)) * pow(((m/n)*t1 - t2), 2)`\r\n",
        "#        This combines the variance for 2 blocks: block 1 having `n` elements with `variance1` and a sum of `t1` and block 2 having `m` elements\r\n",
        "#        with `variance2` and a sum of `t2`. The algorithm is proven to be numerically stable but there is a reasonable loss of accuracy (~0.1% error).\r\n",
        "#        Note that collecting minimum and maximum values is reasonably innefficient, adding about 80% to the running time, and hence is disabled by default.\r\n",
        "#     '''\r\n",
        "#     def __init__(self, n_dims:int=2, record_range=False):\r\n",
        "#         self._n_dims,self._range = n_dims,record_range\r\n",
        "#         self.n,self.sum,self.min,self.max = 0,None,None,None\r\n",
        "    \r\n",
        "#     def update(self, data:Tensor):\r\n",
        "\r\n",
        "#         data = data.view(*list(data.shape[:-self._n_dims]) + [-1])\r\n",
        "#         with torch.no_grad():\r\n",
        "#             new_n,new_var,new_sum = data.shape[-1],data.var(-1),data.sum(-1)\r\n",
        "#             if self.n == 0:\r\n",
        "#                 self.n = new_n\r\n",
        "#                 self._shape = data.shape[:-1]\r\n",
        "#                 self.sum = new_sum\r\n",
        "#                 self._nvar = new_var.mul_(new_n)\r\n",
        "#                 if self._range:\r\n",
        "#                     self.min = data.min(-1)[0]\r\n",
        "#                     self.max = data.max(-1)[0]\r\n",
        "#             else:\r\n",
        "#                 # assert data.shape[:-1] == self._shape, f\"Mismatched shapes, expected {self._shape} but got {data.shape[:-1]}.\"\r\n",
        "#                 ratio = self.n / new_n\r\n",
        "#                 t = (self.sum / ratio).sub_(new_sum).pow_(2)\r\n",
        "#                 self._nvar.add_(new_n, new_var).add_(ratio / (self.n + new_n), t)\r\n",
        "#                 self.sum.add_(new_sum)\r\n",
        "#                 self.n += new_n\r\n",
        "#                 if self._range:\r\n",
        "#                     self.min = torch.min(self.min, data.min(-1)[0])\r\n",
        "#                     self.max = torch.max(self.max, data.max(-1)[0])\r\n",
        "\r\n",
        "#     @property\r\n",
        "#     def mean(self): return self.sum / self.n if self.n > 0 else None\r\n",
        "#     @property\r\n",
        "#     def var(self): return self._nvar / self.n if self.n > 0 else None\r\n",
        "#     @property\r\n",
        "#     def std(self): return self.var.sqrt() if self.n > 0 else None\r\n",
        "\r\n",
        "#     def __repr__(self):\r\n",
        "#         def _fmt_t(t:Tensor):\r\n",
        "#             if t.numel() > 5: return f\"tensor of ({','.join(map(str,t.shape))})\"\r\n",
        "#             def __fmt_t(t:Tensor):\r\n",
        "#                 return '[' + ','.join([f\"{v:.3g}\" if v.ndim==0 else __fmt_t(v) for v in t]) + ']'\r\n",
        "#             return __fmt_t(t)\r\n",
        "#         rng_str = f\", min={_fmt_t(self.min)}, max={_fmt_t(self.max)}\" if self._range else \"\"\r\n",
        "#         return f\"RunningStatistics(n={self.n}, mean={_fmt_t(self.mean)}, std={_fmt_t(self.std)}{rng_str})\"\r\n",
        "\r\n",
        "# def collect_stats(items:Iterable, n_dims:int=2, record_range:bool=False):\r\n",
        "#     stats = RunningStatistics(n_dims, record_range)\r\n",
        "#     for it in progress_bar(items.next()):\r\n",
        "#         it = it.float()\r\n",
        "#         if hasattr(it, 'data'):\r\n",
        "#             stats.update(it.data)\r\n",
        "#         else:\r\n",
        "#             stats.update(it)\r\n",
        "#     return stats\r\n",
        "\r\n",
        "# dd = RunningStatistics\r\n",
        "# stats = collect_stats(dataiter)\r\n",
        "# stats\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZXTf5UJDsbO",
        "outputId": "37e138e7-9b65-4653-d0f3-ab3d22356726"
      },
      "source": [
        "# stats.mean.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5057, 0.5057, 0.5057])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGmx2VtMJKQi",
        "outputId": "be0872a5-6dcd-43c5-e056-bc4732399513"
      },
      "source": [
        "# stats.std.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1902, 0.1902, 0.1902])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOCxit_OJ787"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}